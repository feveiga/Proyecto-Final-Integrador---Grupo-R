# -*- coding: utf-8 -*-
"""TP Integrador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w4B5r6xWCQw_hxGfO57H8YF-f_AULdrv

1. EDA
   1. Variables: selecci√≥n, unificaci√≥n y agregado de 'Type'
   2. Productos: descarte seg√∫n calidad de la serie
   3. Outliers: identificaci√≥n
   4. Transformaci√≥n para modelado: descomposici√≥n, normalizaci√≥n y estructuraci√≥n secuencial
2. MODELOS
   1. XGBoost Global: entrenamiento, validaci√≥n por expanding window y evaluaci√≥n
   2. XGBoost por producto
   3. ARIMA
3. PRODUCTO
   1. Selecci√≥n del modelo
   2. Confecci√≥n del producto
   3. ¬øAn√°lisis de impacto esperado?
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

# Commented out IPython magic to ensure Python compatibility.
#Importaci√≥n de Librerias
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Configuraciones para visualizaci√≥n
# %matplotlib inline
sns.set(style="whitegrid")

#...Agregar librerias a medida que sean necesarias

#Importaci√≥n de datos desde Kaggle
import kagglehub
from kagglehub import KaggleDatasetAdapter

#Omisi√≥n de warnings
import warnings
warnings.filterwarnings('ignore')
from statsmodels.tools.sm_exceptions import InterpolationWarning
warnings.simplefilter('ignore', InterpolationWarning)

#Modelado
import xgboost as xgb
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score
from statsmodels.tsa.seasonal import STL # Season-Trend decomposition using LOESS.
from statsmodels.tsa.stattools import adfuller, kpss  # ADF test (Augmented Dickey-Fuller) para no-estacionariedad y
                                                      # KPSS test (Kwiatkowski-Phillips-Schmidt-Shin) para estacionariedad. Complementarios
import joblib
#import pmdarima as pm #problemas de versiones con otras librerias (numpy)
from tqdm import tqdm

#Importacion de Dataset
dataset_id = "ramkrijal/agriculture-vegetables-fruits-time-series-prices"
csv_file = "kalimati_tarkari_dataset.csv"

try:
    df_vege_fruits = kagglehub.load_dataset(
        KaggleDatasetAdapter.PANDAS,
        dataset_id,
        csv_file
    )
    print("Dataset cargado correctamente.")
except Exception as e:
    print("Error al cargar el dataset:", e)

"""#1. EDA
1. Variables
2. Productos
3. Outliers
4. Transformaci√≥n para modelado

##1.1 VARIABLES
"""

df_vege_fruits.head()

df_vege_fruits.info()

# Convertimos la columna Date a formato datetime
df_vege_fruits['Date'] = pd.to_datetime(df_vege_fruits['Date'])

# Revisamos los tipos nuevamente
df_vege_fruits.dtypes

# Eliminar columnas que no usaremos: n√∫mero de serie porque no aporta, m√≠nimo y m√°ximo porque vamos a restringir la complejidad del an√°lisis.
df_vege_fruits.drop(columns=['SN','Minimum','Maximum'], inplace=True)

df_vege_fruits.describe(include='all')

# Conteo de valores nulos por columna
df_vege_fruits.isnull().sum()

# Duplicados
df_vege_fruits.duplicated().sum()

# Unidades
df_vege_fruits['Unit'].unique()

# Unificamos KG y Kg porque son la misma unidad
df_vege_fruits['Unit'] = df_vege_fruits['Unit'].replace({'KG':'Kg'})

# Productos
df_vege_fruits['Commodity'].unique()

df_vege_fruits['Commodity'].nunique()

# Separamos en categor√≠as de productos

verduras = [
    "Tomato Big(Nepali)", "Tomato Small(Local)", "Tomato Small(Tunnel)",
    "Tomato Big(Indian)", "Tomato Small(Indian)", "Tomato Small(Terai)",
    "Potato Red", "Potato White", "Potato Red(Indian)", "Potato Red(Mude)",
    "Onion Dry (Indian)", "Onion Dry (Chinese)",
    "Carrot(Local)", "Carrot(Terai)",
    "Cabbage(Local)", "Cabbage(Terai)", "Cabbage", "Red Cabbbage",
    "Cauli Local", "Cauli Local(Jyapu)", "Cauli Terai",
    "Raddish Red", "Raddish White(Local)", "Raddish White(Hybrid)",
    "Brinjal Long", "Brinjal Round",
    "Cow pea(Long)", "Cowpea(Short)",
    "Green Peas",
    "French Bean(Local)", "French Bean(Hybrid)", "French Bean(Rajma)",
    "Soyabean Green",
    "Bitter Gourd", "Bottle Gourd", "Pointed Gourd(Local)", "Pointed Gourd(Terai)",
    "Snake Gourd", "Smooth Gourd", "Sponge Gourd", "Pumpkin",
    "Squash(Long)", "Squash(Round)",
    "Turnip", "Turnip A",
    "Okara", "Christophine",
    "Brd Leaf Mustard", "Mustard Leaf", "Spinach Leaf", "Cress Leaf", "Fenugreek Leaf",
    "Onion Green",
    "Ginger",
    "Chilli Dry", "Chilli Green", "Chilli Green(Bullet)", "Chilli Green(Machhe)", "Chilli Green(Akbare)",
    "Capsicum",
    "Garlic Green", "Garlic Dry Chinese", "Garlic Dry Nepali",
    "Clive Dry", "Clive Green", "Coriander Green",
    "Asparagus", "Neuro", "Brocauli", "Sugarbeet", "Drumstick",
    "Lettuce", "Celery", "Parseley", "Fennel Leaf", "Mint",
    "Bamboo Shoot", "Arum", "Maize", "Sword Bean",
    "Yam", "Sweet Potato", "Knolkhol",
    "Cucumber(Local)", "Cucumber(Hybrid)", "Bauhania flower"
]

frutas = [
    "Apple(Jholey)", "Apple(Fuji)",
    "Banana",
    "Lime", "Lemon", "Sweet Lime",
    "Orange(Nepali)", "Orange(Indian)", "Sweet Orange", "Mandarin", "Kinnow",
    "Grapes(Green)", "Grapes(Black)",
    "Pomegranate",
    "Mango(Maldah)", "Mango(Dushari)", "Mango(Calcutte)", "Mango(Chousa)",
    "Water Melon(Green)", "Water Melon(Dotted)", "Musk Melon",
    "Jack Fruit",
    "Papaya(Nepali)", "Papaya(Indian)",
    "Sugarcane",
    "Pineapple",
    "Guava", "Mombin", "Barela", "Bakula",
    "Pear(Local)", "Pear(Chinese)",
    "Litchi(Local)", "Litchi(Indian)",
    "Kiwi", "Strawberry"
]

otros = ["Tofu", "Gundruk", "Tamarind", "Mushroom(Kanya)",
         "Mushroom(Button)", "Fish Fresh", "Fish Fresh(Rahu)",
         "Fish Fresh(Bachuwa)", "Fish Fresh(Chhadi)", "Fish Fresh(Mungari)"]

print('Verduras: ',len(verduras))
print('Frutas: ',len(frutas))
print('Otros: ',len(otros))
print('Total: ',len(verduras)+len(frutas)+len(otros))

# Definimos la funci√≥n de clasificaci√≥n
def clasificar_producto(prod):
    if prod in verduras:
        return "Vegetable"
    elif prod in frutas:
        return "Fruit"
    elif prod in otros:
        return "Other"
    else:
        return "Unknown"

# Creamos una nueva columna para la categor√≠a
df_vege_fruits["Type"] = df_vege_fruits["Commodity"].apply(clasificar_producto)

df_vege_fruits.info()

df_vege_fruits.head()

"""##1.1 PRODUCTOS"""

#Cantidad de datos por producto
pd.set_option('display.max_rows', None)
display(df_vege_fruits['Commodity'].value_counts())
pd.reset_option('display.max_rows')

"""Quitamos los que tienen menos de 200 entradas"""

N_min = 200

# Obtener la cantidad de datos por producto
commodity_counts = df_vege_fruits['Commodity'].value_counts()

# Separar los productos con m√°s de N_min registros
commodities_more_than_N_min = commodity_counts[commodity_counts > N_min].index.tolist()

# Separar los productos con N_min registros o menos
commodities_N_min_or_less = commodity_counts[commodity_counts <= N_min].index.tolist()

print(f"Productos con m√°s de {N_min} registros:")
display(commodities_more_than_N_min)

print(f"\nProductos con {N_min} registros o menos:")
display(commodities_N_min_or_less)

num_commodities_more_than_N_min = len(commodities_more_than_N_min)
print(f"Hay {num_commodities_more_than_N_min} productos con m√°s de {N_min} registros.")

df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_N_min_or_less)]
df_vege_fruits['Commodity'].nunique()

"""Revisamos la distribuci√≥n de d√≠as entre entradas para identificar anomal√≠as"""

# ordenamos por producto y fecha
df_vege_fruits = df_vege_fruits.sort_values(["Commodity", "Date"])

# diferencias entre fechas dentro de cada producto
df_aux = df_vege_fruits.copy()
df_aux["delta_dias"] = (
    df_vege_fruits.groupby("Commodity")["Date"].diff().dt.days  #total_seconds() / (24*3600)
)

# intervalo medio por producto
intervalo_medio = df_aux.groupby("Commodity")["delta_dias"].mean()

"""Quitamos los que tienen una frecuencia promedio menor a una entrada por semana"""

# productos v√°lidos: intervalo medio <= 7 d√≠as
productos_validos = intervalo_medio[intervalo_medio <= 7].index

# filtramos el dataframe original
df_vege_fruits = df_vege_fruits[df_vege_fruits["Commodity"].isin(productos_validos)]
df_vege_fruits['Commodity'].nunique()

df_aux['delta_dias'].describe()
# valor m√°ximo 2054 y desv√≠o 11.9!!!

#intervalo_medio.head()
df_aux.sort_values(by='delta_dias', ascending=False)#.head(10)

df_aux.isnull().sum()
# est√° bien, parecen ser el principio y final de la serie porque hay
# uno por producto (al calcular las diferencias queda con una entrada menos)

# Distribuci√≥n de intervalos global para intervalos grandes

lower_limit = 30
tmp_df = df_aux[df_aux['delta_dias'] > lower_limit]
#x = np.linspace(0, 2100, 11)
plt.figure(figsize=(14, 4))
plt.hist(tmp_df['delta_dias'],bins=300)
plt.title("Acumulado de todos los productos")
plt.xlabel(f"Diferencia de d√≠as entre entradas (>{lower_limit})")
#plt.xticks(x)
plt.tight_layout()
plt.show()

"""Quitamos los productos que tienen periodos faltantes m√°s largos que un fuera de temporada estimado"""

# asumiendo al menos 2 meses de temporada, fuera de temporada: 10x30 = 300
tmp_df = df_aux[df_aux['delta_dias'] > 300]
tmp_df.describe(include='all')

commodities_big_gap = tmp_df['Commodity'].unique()
commodities_big_gap

# Revisamos las curvas
for commodity in commodities_big_gap:
    plt.figure(figsize=(14, 4))
    temp_df = df_vege_fruits[df_vege_fruits['Commodity'] == commodity]
    plt.scatter(temp_df['Date'], temp_df['Average'], label=commodity, marker='.', s=5)
    plt.legend()
    #plt.title("Evoluci√≥n de precios promedios")
    plt.xlabel("Fecha")
    plt.ylabel("Precio promedio por unidad de medida")
    plt.tight_layout()
    plt.show()

# Quitamos esos productos por tener series muy irregulares o con pocos datos hacia el final
df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_big_gap)]

# actualizamos intervalo medio por producto
df_aux = df_aux[~df_aux["Commodity"].isin(commodities_big_gap)]
intervalo_medio = df_aux.groupby("Commodity")["delta_dias"].mean()

df_vege_fruits['Commodity'].nunique()

# Volvemos a revisar la distribuci√≥n de intervalos total

#lower_limit = 5
tmp_df = df_aux[df_aux['delta_dias'] > lower_limit]
#x = np.linspace(0, 2100, 11)
plt.figure(figsize=(14, 4))
plt.hist(tmp_df['delta_dias'],bins=300)
plt.title("Acumulado de todos los productos")
plt.xlabel(f"Diferencia de d√≠as entre entradas (>{lower_limit})")
#plt.xticks(x)
plt.tight_layout()
plt.show()

"""Identificamos los que tienen una frecuencia promedio menor a una entrada cada tres d√≠as"""

# productos a revisar: 3 d√≠as < intervalo medio <= 7 d√≠as
productos_a_revisar = intervalo_medio[(3 < intervalo_medio) & (intervalo_medio <= 7)].index

# d√≠as promedio entre entradas
intervalo_medio[productos_a_revisar]

# Las curvas no parecen mal as√≠ que no los quitamos

plt.figure(figsize=(14, 5))
for commodity in productos_a_revisar:
    temp_df = df_vege_fruits[df_vege_fruits['Commodity'] == commodity]
    plt.scatter(temp_df['Date'], temp_df['Average'], label=commodity, marker='.', s=5)

plt.legend()
plt.title("Evoluci√≥n de precios promedios")
plt.xlabel("Fecha")
plt.ylabel("Precio promedio por unidad de medida")
plt.tight_layout()
plt.show()

"""Revisamos la cantidad de entradas que tiene cada producto en el √∫ltimo a√±o"""

N_days = 365

# calcular fecha de corte
fecha_max = df_vege_fruits["Date"].max()
fecha_corte = fecha_max - pd.Timedelta(days=N_days)

# filtrar √∫ltimos N_days d√≠as
df_ultimos_N_days = df_vege_fruits[df_vege_fruits["Date"] >= fecha_corte]

# contar entradas por producto
conteo_por_producto = df_ultimos_N_days.groupby("Commodity").size().reset_index(name="conteo")

pd.set_option('display.max_rows', None)
display(conteo_por_producto)
pd.reset_option('display.max_rows')

"""Quitamos los que tienen menos de 1 dato cada 3 d√≠as en el √∫ltimo a√±o, dos a√±os, tres a√±os y cuatro a√±os."""

days_in_year = 365

for i in [1,2,3,4]:
  # calcular fecha de corte
  N_days = i*days_in_year
  fecha_corte = fecha_max - pd.Timedelta(days=N_days)

  # filtrar √∫ltimos N_days d√≠as
  df_ultimos_N_days = df_vege_fruits[df_vege_fruits["Date"] >= fecha_corte]

  # contar entradas por producto
  conteo_por_producto = df_ultimos_N_days.groupby("Commodity").size().reset_index(name="conteo")

  # quitar los productos con pocas entradas el √∫ltimo periodo
  min_entries = 120*i
  commodities_no_resent_data = conteo_por_producto[conteo_por_producto['conteo'] < min_entries]['Commodity']
  df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_no_resent_data)]

df_vege_fruits['Commodity'].nunique()

# Volvemos a revisar la cantidad de datos por producto
pd.set_option('display.max_rows', None)
display(df_vege_fruits['Commodity'].value_counts())
pd.reset_option('display.max_rows')

"""##1.3 OUTLIERS
Exploramos los outliers seg√∫n IQR. Como los modelos que vamos a usar son sensibles frente outliers (ARIMA m√°s que nada) usaremos STL, que cuenta con un m√©todo robusto ante outliers, para descomponer las series previamente. Los dejamos por el momento y retomamos para los productos en los que sea necesario.
"""

# Estad√≠stica descriptiva por producto
variables = ['Average'] #['Minimum', 'Maximum', 'Average']
df_vege_fruits.groupby('Commodity')[variables].describe()

# gr√°fico interactivo con dropdown de producto
fig = px.scatter(
    df_vege_fruits,
    x="Date",
    y="Average",
    color="Commodity",            # esto permite tener varios productos
    title="Evoluci√≥n de precios por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evoluci√≥n de precios ‚Äî {prod}"}
                    ],
                }
                for prod in df_vege_fruits["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

# Ver cu√°ntas combinaciones hay de Producto + Tipo + Unidad
df_vege_fruits.groupby(['Commodity', 'Type', 'Unit']).size().reset_index(name='count').head(20)

cant_verduras = 20 # Se mostraran solo esta cantidad para que el grafico sea legible

df_veg = df_vege_fruits[df_vege_fruits['Type'] == 'Vegetable']
top_veg = df_veg['Commodity'].value_counts().nlargest(cant_verduras).index

df_top_veg = df_veg[df_veg['Commodity'].isin(top_veg)]
#df_top_veg = df_veg   #Descomentar para mostrar Todas las verduras

fig = px.box(
    df_top_veg,
    x="Average",
    y="Commodity",
    color="Commodity",
    title="Distribuci√≥n de Precios de Verduras",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Verduras"}
)
fig.show()

df_fruit = df_vege_fruits[
    (df_vege_fruits['Type'] == 'Fruit') & (df_vege_fruits['Unit'] == 'Kg')
]

fig = px.box(
    df_fruit,
    x="Average",
    y="Commodity",
    facet_col="Unit",   # panel por unidad de medida
    color="Commodity",
    title="Distribuci√≥n de Precios de Frutas",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Frutas"}
)
fig.show()

df_fruit = df_vege_fruits[
(df_vege_fruits['Type'] == 'Fruit') & (df_vege_fruits['Unit'] != 'Kg')]

fig = px.box(
    df_fruit,
    x="Average",
    y="Commodity",
    facet_col="Unit",   # panel por unidad de medida
    color="Commodity",
    title="Distribuci√≥n de Precios de Frutas",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Frutas"}
)
fig.show()

df_other = df_vege_fruits[df_vege_fruits['Type'] == 'Other']

fig = px.box(
    df_other,
    x="Average",
    y="Commodity",
    color="Commodity",
    title="Distribuci√≥n de Precios Otros ",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Otros"}
)
fig.show()

# Identificaci√≥n de OUTLIERS con IQR
def identify_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filtrar outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Identificaci√≥n de OUTLIERS con Z-Score
def identify_outliers_zscore(df,column):
    # funci√≥n
    return 0

# Identificaci√≥n de OUTLIERS con IQR diferenciando por producto
outliers_by_comm = {}
columns = ['Average'] # ['Minimum', 'Maximum', 'Average']

for product in df_vege_fruits['Commodity'].unique():
    df_comm = df_vege_fruits[df_vege_fruits['Commodity'] == product]
    for col in columns:
        outliers_df = identify_outliers_iqr(df_comm, col)
        if not outliers_df.empty:
            outliers_by_comm[(product, col)] = outliers_df
            print(f"Outliers detectados en '{col}' para '{product}': {outliers_df.shape[0]}")
            # Opcional: mostrar algunos ejemplos
            #print(outliers_df[['Commodity','Type', 'Date', col]].head())

"""##1.4 TRANSFORMACI√ìN PARA MODELADO

Los modelos que vamos a usar s√≥lo sirven para series estacionarias sin estacionalidad. Hay que quitarles las tendencias y la estacionalidad a las series previamente.

* XGBoost: para que pueda tomar series temporales hay que estructurar los datos.
* ARIMA: tambi√©n se podr√≠a probar usar SARIMA para las series que tengan fuerte estacionalidad (sin quitarla previamente).

###Pruebas de estacionareidad ADF y KPSS:

ADF:   p < 0.05 ‚Üí estacionaria.

KPSS:  p < 0.05 ‚Üí NO estacionaria.

Para el mismo criterio de valor de p (0.05):

*   Caso 1: Ambos tests concluyen que la serie no es estacionaria ‚Üí La serie no es estacionaria.

*   Caso 2: Ambos tests concluyen que la serie es estacionaria ‚Üí La serie es estacionaria.

*   Caso 3: El test KPSS indica estacionariedad y el test ADF indica no estacionariedad ‚Üí La serie es estacionaria en tendencia (trend stationary).
Debe eliminarse la tendencia para que la serie sea estrictamente estacionaria.
Luego se verifica la estacionariedad de la serie detrendida.

*   Caso 4: El test KPSS indica no estacionariedad y el test ADF indica estacionariedad ‚Üí La serie es estacionaria en diferencias (difference stationary).
Debe aplicarse diferenciaci√≥n para hacer la serie estacionaria.
Despu√©s se verifica la estacionariedad de la serie diferenciada.
"""

# Funci√≥n para descomponer una serie: original = tendencia + estacional + residuo
def seasonality_LOESS(serie, period=None, robust=False):
    stl = STL(serie.dropna(), period=period)
    decomposition = stl.fit()

    var_resid = np.var(decomposition.resid)
    var_seasonal_resid = np.var(decomposition.resid + decomposition.seasonal)
    strength = max(0, 1 - var_resid/var_seasonal_resid)

    return decomposition, strength

# Cargar los datos
df = df_vege_fruits.copy()
productos_seleccionados = df['Commodity'].unique().tolist()

# Analizar tendencia y estacionalidad por producto
comportamiento = {}
for producto in productos_seleccionados:
    serie_producto = df[df['Commodity']==producto].set_index("Date")["Average"]

    # Descomposici√≥n tendencia+estacionalidad+residuo
    try:
        decomposition, strength = seasonality_LOESS(serie_producto, robust=True)
        print(f'STL auto-period for {producto}')
    except:
        decomposition, strength = seasonality_LOESS(serie_producto, robust=True, period=365)
        #print(f'STL period 365d for {producto}')

    # Estacionariedad antes y despu√©s de descomponer
    estacionariedad_ADF_prev_STL = adfuller(serie_producto.dropna())
    estacionariedad_ADF_post_STL = adfuller(decomposition.resid)
    estacionariedad_KPSS_prev_STL = kpss(serie_producto.dropna())
    estacionariedad_KPSS_post_STL = kpss(decomposition.resid)

    # Resultados
    comportamiento[producto] = {
        'decomposition': decomposition,
        'strength': strength,
        'adf_pvalue_before_stl': estacionariedad_ADF_prev_STL[1],
        'adf_pvalue_after_stl': estacionariedad_ADF_post_STL[1],
        'kpss_pvalue_before_stl': estacionariedad_KPSS_prev_STL[1],
        'kpss_pvalue_after_stl': estacionariedad_KPSS_post_STL[1]
    }

# Pasar el dict a DataFrame
df_comportamiento = pd.DataFrame.from_dict(comportamiento, orient="index")

# Resetear √≠ndice para que 'Commodity' quede como columna
df_comportamiento = df_comportamiento.reset_index().rename(columns={"index": "Commodity"})

# Filtrar solo las columnas de inter√©s
df_comportamiento = df_comportamiento[[
    "Commodity",
    "strength",
    "adf_pvalue_before_stl",
    "adf_pvalue_after_stl",
    "kpss_pvalue_before_stl",
    "kpss_pvalue_after_stl"
]]

# Estacionalidad
# strength ~ 1: fuerte estacionalidad
full_print=False
plot=False

# Identificar productos con estacionalidad
# El valor de strength fue elegido arbitrariamente: buscar alg√∫n criterio!
series_sin_estacionalidad = df_comportamiento[df_comportamiento['strength'] < 0.5]
series_con_estacionalidad = df_comportamiento[df_comportamiento['strength'] >= 0.5]
N_series_with_seas = series_con_estacionalidad['Commodity'].value_counts().sum()

print(f'Cantidad de series CON estacionalidad = {N_series_with_seas}')
if full_print:
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print(series_con_estacionalidad.sort_values(by='strength', ascending=False))
else:
    print(series_con_estacionalidad.sort_values(by='strength', ascending=False))

# Gr√°ficos de las componentes de la serie (original=trend+seasonal+residual)
if plot:
    for producto in productos_seleccionados:
        decomposition = comportamiento[producto]['decomposition']
        strength = comportamiento[producto]['strength']
        adf_pvalue_before = comportamiento[producto]['avf_pvalue_before_stl']
        adf_pvalue_after = comportamiento[producto]['avf_pvalue_after_stl']
        kpss_pvalue_before = comportamiento[producto]['kpss_pvalue_before_stl']
        kpss_pvalue_after = comportamiento[producto]['kpss_pvalue_after_stl']

        print(f'\n{producto}: strength = {strength}')
        print(f'adf-p-before = {adf_pvalue_before}\t adf-p-after = {adf_pvalue_after}\t kpss-p-before = {kpss_pvalue_before}\t kpss-p-after = {kpss_pvalue_after}')
        decomposition.plot()
        plt.show()

# Estacionariedad
# ADF   p < 0.05 ‚Üí estacionaria.    p < 0.025 m√°s conservador (m√°s probabilidad de requerir transformaci√≥n)
# KPSS  p < 0.05 ‚Üí NO estacionaria. p < 0.075 m√°s conservador (m√°s probabilidad de requerir transformaci√≥n)
full_print=False

# Identificar series NO estacionarias ANTES de la descomposici√≥n
series_adf_no_estacionarias_antes = df_comportamiento[df_comportamiento['adf_pvalue_before_stl'] >= 0.05]
N_series_adf_non_stat_before = series_adf_no_estacionarias_antes['Commodity'].value_counts().sum()
series_kpss_no_estacionarias_antes = df_comportamiento[df_comportamiento['kpss_pvalue_before_stl'] < 0.05]
N_series_kpss_non_stat_before = series_kpss_no_estacionarias_antes['Commodity'].value_counts().sum()

# Identificar series NO estacionarias DESPU√âS de la descomposici√≥n
series_adf_no_estacionarias_despues = df_comportamiento[df_comportamiento['adf_pvalue_after_stl'] >= 0.05]
N_series_adf_non_stat_after = series_adf_no_estacionarias_despues['Commodity'].value_counts().sum()
series_kpss_no_estacionarias_despues = df_comportamiento[df_comportamiento['kpss_pvalue_after_stl'] < 0.05]
N_series_kpss_non_stat_after = series_adf_no_estacionarias_despues['Commodity'].value_counts().sum()

# Imprimir resultados
print(f'Series NO estacionarias pre-STL = {N_series_adf_non_stat_before} (ADF)\t {N_series_kpss_non_stat_before} (KPSS)')
print(f'Series NO estacionarias pos-STL = {N_series_adf_non_stat_after} (ADF)\t {N_series_adf_non_stat_after} (KPSS)')
if full_print:
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print(series_adf_no_estacionarias_antes.sort_values(by='adf_pvalue_before_stl', ascending=False))
        print(series_kpss_no_estacionarias_antes.sort_values(by='kpss_pvalue_before_stl', ascending=False))
else:
    print(series_adf_no_estacionarias_antes.sort_values(by='adf_pvalue_before_stl', ascending=False))
    print(series_kpss_no_estacionarias_antes.sort_values(by='kpss_pvalue_before_stl', ascending=False))

"""Ac√° habr√≠a que transformar las series si hiciera falta hacerlas estacionarias luego de quitarles la tendencia y estacionalidad. Parece que no va a ser necesario porque todas las series transformadas (resid) son estacionarias a nivel de significancia 0.05.

Nota: KPSS s√≥lo puede reportar p-valores menores a 0.1, mientras usemos nivel de significancia menor a eso estamos bien.

Ahora agregamos las tres descomposiciones al dataframe.
"""

rows = []
for producto in productos_seleccionados:
    serie_producto = df[df['Commodity']==producto].set_index("Date")["Average"]
    seasonal = comportamiento[producto]['decomposition'].seasonal
    trend = comportamiento[producto]['decomposition'].trend
    resid = comportamiento[producto]['decomposition'].resid

    tmp = pd.DataFrame({
        "Date": serie_producto.index,
        "Commodity": producto,
        "seasonal": seasonal.values,
        "trend": trend.values,
        "resid": resid.values
    })
    rows.append(tmp.reset_index(drop=True))

temp_df = pd.concat(rows, ignore_index=True)
temp_df.info()

print(df[["Commodity","Date"]].duplicated().sum())        # deber√≠a ser 0
print(temp_df[["Commodity","Date"]].duplicated().sum())   # deber√≠a ser 0

df_full = df.merge(
    temp_df,
    on=["Commodity", "Date"],
    how="left" #"inner", usar "left" si quer√©s conservar todas las filas de df
)

df_full.info()

# gr√°fico interactivo con dropdown de producto
fig = px.scatter(
    df_full,
    x="Date",
    y="resid",
    color="Commodity",            # esto permite tener varios productos
    title="Evoluci√≥n de residuos por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evoluci√≥n de residuos ‚Äî {prod}"}
                    ],
                }
                for prod in df_full["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

# Funci√≥n que adapta la base de datos para poder usar modelos de ML no
# espec√≠ficos de series temporales
def estructurar_datos(data, target, lags=[1, 2, 3, 7, 14, 30], low_cat_feat=None):
    df_feat = data.copy()

    # El modelo no maneja fechas, derivamos variables num√©ricas
    # para mayor sensibilidad a estacionalidades
    df_feat["dayofyear"] = df_feat["Date"].dt.dayofyear
    df_feat["dayofweek"] = df_feat["Date"].dt.dayofweek
    df_feat["month"] = df_feat["Date"].dt.month
    df_feat["year"] = df_feat["Date"].dt.year

    # One-hot encoding para categ√≥ricas de poco rango que potencialmente
    # puedan aportar al entrenamiento
    if low_cat_feat is not None:
        for ft in low_cat_feat:
            df_feat = pd.get_dummies(df_feat, columns=[ft], drop_first=True)
            print(f'\'{ft}\' one-hot encoded')

    # Target encoding para productos as√≠ los tiene en cuenta
    mean_prices = df_feat.groupby("Commodity")[target].mean()
    df_feat["Commodity_t_encoded"] = df_feat["Commodity"].map(mean_prices)
    print('\'Commodity\' mean-encoded into \'Commodity_t_encoded\'')

    # Normalizaci√≥n para estabilizar escalas
    stats = df_feat.groupby("Commodity")[target].agg(["mean", "std"])
    df_feat = df_feat.merge(stats, on="Commodity")
    df_feat[target+"_norm"] = (df_feat[target] - df_feat["mean"]) / df_feat["std"]
    print(f'\'{target}\' normalized into \'{target}_norm\'')

    # Se agregan variables con corrimiento y medias m√≥viles
    # que le dan estructura secuencial a los datos.
    for lag in lags:
        df_feat[f"lag_{lag}"] = df_feat.groupby("Commodity")[target+"_norm"].shift(lag)
        print(f'Added lag_{lag}')

        df_feat[f"roll_mean_{lag}"] = (
            df_feat.groupby("Commodity")[target+"_norm"]
            .shift(1)
            .rolling(window=lag)
            .mean()
        )
        print(f'Added roll_mean_{lag}')

    df_feat = df_feat.dropna()

    return df_feat

# Crear las features
df_feat = estructurar_datos(
    df_full,
    'resid',
    low_cat_feat=['Unit','Type']
)

df_feat.info()

# gr√°fico interactivo con dropdown de producto
fig = px.scatter(
    df_feat,
    x="Date",
    y="resid_norm",
    color="Commodity",            # esto permite tener varios productos
    title="Evoluci√≥n de residuos normalizados por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evoluci√≥n de residuos normalizados ‚Äî {prod}"}
                    ],
                }
                for prod in df_full["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

"""#2. MODELOS
1. XGBoost Global
2. XGBoost por producto
3. ARIMA

Consideraciones:
* Hacer entrenamiento y validaci√≥n por expanding window con X_train_valid / y_train_valid.

* Evaluaci√≥n con X_test / y_test.

* Siempre invertir las transformaciones antes de calcular m√©tricas interpretables (RMSE, MAE, MAPE, R¬≤): desnormalizar usando mean y std (df_feat) y agregarles tendencia y estacionalidad (trend y seasonal en df_feat) . Para series con ceros o valores muy peque√±os, usar sMAPE en lugar de MAPE. Combinar m√©tricas es lo ideal:

  * RMSE ‚Üí magnitud del error
  * MAPE/sMAPE ‚Üí error relativo
  * MFE ‚Üí sesgo del modelo

Opcional: En la validaci√≥n y evaluaci√≥n se puede usar 'Average' como y_true para el c√°lculo de los errores y as√≠ alcanza con s√≥lo invertir las transformaciones de y_pred. Se tomar√≠a de df_feat y har√≠a falta repetir los splits.
"""

# Separar en entrenamiento+validaci√≥n y evaluaci√≥n (√∫ltimos 12 meses)
fecha_corte = df_feat["Date"].max() - pd.DateOffset(months=12)

train_valid = df_feat[df_feat["Date"] < fecha_corte]
test = df_feat[df_feat["Date"] >= fecha_corte]

# Separar variables explicativas de variable objetivo
X_train_valid = train_valid.drop(columns=["Average","resid","resid_norm","trend","seasonal","mean","std"])
y_train_valid = train_valid[["Commodity", "resid_norm"]]

X_test = test.drop(columns=["Average","resid","resid_norm","trend","seasonal","mean","std"])
y_test = test[["Commodity","resid_norm"]]

# Funciones auxiliares
def f_smape(y_true, y_pred):
    """Symmetric Mean Absolute Percentage Error"""
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-8)) * 100

def f_mfe(y_true, y_pred):
    """Mean Forecast Error (sesgo)"""
    return np.mean(y_pred - y_true)

def invertir_transformaciones(y_pred, df_ref):
    """Invertir desnormalizaci√≥n y sumar tendencia + estacionalidad"""
    y_pred_1d = np.array(y_pred).ravel()  # Asegura vector 1D
    y_inv = (y_pred_1d * df_ref["std"].values) + df_ref["mean"].values
    y_inv += df_ref["trend"].values + df_ref["seasonal"].values
    return y_inv

def mape_condicional(y_true, y_pred, umbral_min=1.0):
    """
    Calcula MAPE o sMAPE seg√∫n el umbral m√≠nimo de valor real.
    Si alg√∫n valor < umbral_min -> sMAPE, sino MAPE
    """
    if np.any(y_true < umbral_min):
        return f_smape(y_true, y_pred)  # sMAPE
    else:
        return mean_absolute_percentage_error(y_true, y_pred) # np.mean(np.abs((y_pred - y_true) / y_true)) * 100  # MAPE

"""##2.1 XGBoost Global"""

# OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS (Optuna + TimeSeriesSplit)
X = X_train_valid.drop(columns=["Commodity","Date"])
y = y_train_valid.drop(columns=["Commodity"])

N_FOLDS = 7
N_TRIALS = 30

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 700),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "subsample": trial.suggest_float("subsample", 0.7, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "random_state": 42,
        "n_jobs": -1,
        "objective": "reg:squarederror",
        "eval_metric": "rmse"
    }

    tscv = TimeSeriesSplit(n_splits=N_FOLDS)
    scores = []

    for train_idx, valid_idx in tscv.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)

        y_pred = model.predict(X_valid)
        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
        scores.append(rmse)

    return np.mean(scores)

print("üîé Buscando hiperpar√°metros √≥ptimos con Optuna...")
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=N_TRIALS)

best_params = study.best_params
print(f"\nüèÜ Mejores par√°metros: {best_params}")
print(f"‚úÖ RMSE medio en validaci√≥n: {study.best_value:.4f}")

# ===============================================================
# ENTRENAMIENTO FINAL CON LOS MEJORES PAR√ÅMETROS
# ===============================================================
model_final = xgb.XGBRegressor(**best_params)
model_final.fit(X, y)

# Guardar modelo global
joblib.dump(model_final, "xgb_model_global_opt.pkl")

# ===============================================================
# EVALUACI√ìN EN TEST
# ===============================================================
X_test_model = X_test.drop(columns=["Commodity","Date"])
y_pred_test = model_final.predict(X_test_model)

# Invertir transformaciones a escala original
y_pred_inv = invertir_transformaciones(y_pred_test, test)
y_true_inv = invertir_transformaciones(y_test.drop(columns="Commodity"), test)

# ===============================================================
# M√âTRICAS GLOBALES
# ===============================================================
rmse_global = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))
mape_global = mape_condicional(y_true_inv, y_pred_inv)
r2_global = r2_score(y_true_inv, y_pred_inv)
mfe_global = f_mfe(y_true_inv, y_pred_inv)

print("\n===== Desempe√±o Global en Test =====")
print(f"RMSE: {rmse_global:.3f}")
print(f"MAPE/sMAPE: {mape_global:.3f}")
print(f"R¬≤: {r2_global:.3f}")
print(f"MFE: {mfe_global:.3f}")

# ===============================================================
# M√âTRICAS POR PRODUCTO
# ===============================================================
df_eval = test.copy()
df_eval["y_true_inv"] = y_true_inv
df_eval["y_pred_inv"] = y_pred_inv

metricas_prod = []
for prod, grp in df_eval.groupby("Commodity"):
    rmse = np.sqrt(mean_squared_error(grp["y_true_inv"], grp["y_pred_inv"]))
    mape = mape_condicional(grp["y_true_inv"], grp["y_pred_inv"])
    r2 = r2_score(grp["y_true_inv"], grp["y_pred_inv"])
    mfe = f_mfe(grp["y_true_inv"], grp["y_pred_inv"])
    metricas_prod.append({"Commodity": prod, "RMSE": rmse, "MAPE/sMAPE": mape, "R2": r2, "MFE": mfe})

df_metricas_prod = pd.DataFrame(metricas_prod)

# Top y Bottom 10 por RMSE
top10_rmse = df_metricas_prod.sort_values("RMSE").head(10)
bottom10_rmse = df_metricas_prod.sort_values("RMSE").tail(10)

print("\n===== üîù TOP 10 productos con mejor desempe√±o (RMSE) =====")
print(top10_rmse)

print("\n===== üîª BOTTOM 10 productos con peor desempe√±o (RMSE) =====")
print(bottom10_rmse)

print("\n==================================================================")

# Top y Bottom 10 por MAPE/sMAPE
top10_rmse = df_metricas_prod.sort_values("MAPE/sMAPE").head(10)
bottom10_rmse = df_metricas_prod.sort_values("MAPE/sMAPE").tail(10)

print("\n===== üîù TOP 10 productos con mejor desempe√±o (MAPE/sMAPE) =====")
print(top10_rmse)

print("\n===== üîª BOTTOM 10 productos con peor desempe√±o (MAPE/sMAPE) =====")
print(bottom10_rmse)

print("\n==================================================================")

# Top y Bottom 10 por R2
top10_rmse = df_metricas_prod.sort_values("R2").tail(10)
bottom10_rmse = df_metricas_prod.sort_values("R2").head(10)

print("\n===== üîù TOP 10 productos con mejor desempe√±o (R2) =====")
print(top10_rmse)

print("\n===== üîª BOTTOM 10 productos con peor desempe√±o (R2) =====")
print(bottom10_rmse)

# ===============================================================
# GR√ÅFICOS DE SERIES REALES VS PREDICHAS
# ===============================================================
productos = df_eval["Commodity"].unique()
for prod in productos:
    df_p = df_eval[df_eval["Commodity"] == prod]
    plt.figure(figsize=(10, 4))
    plt.plot(df_p["Date"], df_p["y_true_inv"], label="Real", lw=2)
    plt.plot(df_p["Date"], df_p["y_pred_inv"], label="Predicho", lw=2, linestyle="--")
    plt.title(f"{prod} - Predicci√≥n de precios (test)")
    plt.xlabel("Fecha")
    plt.ylabel("Precio")
    plt.legend()
    plt.tight_layout()
    plt.show()

"""##2.2 XGBoost por producto"""

# ==========================================
# 1. PAR√ÅMETROS
# ==========================================
N_FOLDS = 7
UMBRAL_MIN = 1.0  # umbral para usar sMAPE
resultados = {}

# ==========================================
# 2. LOOP POR PRODUCTO
# ==========================================
for producto in sorted(X_train_valid["Commodity"].unique()):
    print(f"\nüîπ Entrenando modelo para: {producto}")

    # Filtrar datos del producto
    X_t_v = X_train_valid[X_train_valid["Commodity"] == producto].copy()
    y_t_v = y_train_valid[y_train_valid["Commodity"] == producto].copy()

    X_t_v = X_t_v.drop(columns=["Date", "Commodity"])
    y_t_v = y_t_v.drop(columns=["Commodity"])

    # Validaci√≥n temporal tipo expanding window
    tscv = TimeSeriesSplit(n_splits=N_FOLDS)
    fold_metrics = []

    for fold, (train_idx, valid_idx) in enumerate(tscv.split(X_t_v)):
        X_train, X_valid = X_t_v.iloc[train_idx], X_t_v.iloc[valid_idx]
        y_train, y_valid = y_t_v.iloc[train_idx], y_t_v.iloc[valid_idx]

        # Modelo XGBoost compatible con Colab
        model = xgb.XGBRegressor(
            n_estimators=400,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="reg:squarederror",
            random_state=42,
            n_jobs=-1,
            #early_stopping_rounds=30, # removed early stopping rounds
            eval_metric="rmse"
        )

        # Entrenamiento
        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            verbose=False
        )

        # Predicci√≥n
        y_pred = model.predict(X_valid)

        # Invertir transformaciones a precio real
        df_ref = train_valid[train_valid["Commodity"] == producto].iloc[valid_idx]
        y_true_inv = invertir_transformaciones(y_valid.values, df_ref)
        y_pred_inv = invertir_transformaciones(y_pred, df_ref)

        # M√©tricas por fold
        rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))
        r2 = r2_score(y_true_inv, y_pred_inv)
        bias = f_mfe(y_true_inv, y_pred_inv)
        error_rel = mape_condicional(y_true_inv, y_pred_inv, UMBRAL_MIN)

        fold_metrics.append({
            "fold": fold,
            "RMSE": rmse,
            "R2": r2,
            "MFE": bias,
            "MAPE/sMAPE": error_rel
        })

        print(f"  Fold {fold}: RMSE={rmse:.2f}, MAPE/sMAPE={error_rel:.2f}, R2={r2:.2f}")

    # Entrenamiento final sobre todo train_valid
    model_final = xgb.XGBRegressor(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        random_state=42,
        n_jobs=-1,
        eval_metric="rmse"
    )
    model_final.fit(X_t_v, y_t_v)

    # Evaluaci√≥n final sobre test
    X_test_prod = X_test[X_test["Commodity"] == producto].drop(columns=["Date", "Commodity"])
    y_test_prod = y_test[y_test["Commodity"] == producto]["resid_norm"]

    y_pred_test = model_final.predict(X_test_prod)
    df_test_ref = test[test["Commodity"] == producto]

    y_true_inv_test = invertir_transformaciones(y_test_prod.values, df_test_ref)
    y_pred_inv_test = invertir_transformaciones(y_pred_test, df_test_ref)

    rmse_test = np.sqrt(mean_squared_error(y_true_inv_test, y_pred_inv_test))
    r2_test = r2_score(y_true_inv_test, y_pred_inv_test)
    bias_test = f_mfe(y_true_inv_test, y_pred_inv_test)
    error_test = mape_condicional(y_true_inv_test, y_pred_inv_test, UMBRAL_MIN)

    # Guardar m√©tricas y modelo
    df_folds = pd.DataFrame(fold_metrics)
    resultados[producto] = {
        "val_RMSE": df_folds["RMSE"].mean(),
        "val_MAPE/sMAPE": df_folds["MAPE/sMAPE"].mean(),
        "test_RMSE": rmse_test,
        "test_MAPE/sMAPE": error_test,
        "test_MFE": bias_test,
        "test_R2": r2_test,
        "modelo": model_final
    }

    # Guardar modelo individual
    joblib.dump(model_final, f"xgb_model_{producto.replace('/', '_')}.pkl")

# ==========================================
# Resumen final
# ==========================================

df_resultados = pd.DataFrame([
    {"Producto": k, **v} for k, v in resultados.items()
])

# Umbral de MAPE/sMAPE
UMBRAL_MAPE = 5.0

# Filtrar products that meet the condition
productos_filtrados = [p for p, r in resultados.items() if r["test_MAPE/sMAPE"] < UMBRAL_MAPE]

print(f"Productos con MAPE/sMAPE < {UMBRAL_MAPE}%: {productos_filtrados}")

# Loop para graficar
for producto_graf in productos_filtrados:
    df_test_ref = test[test["Commodity"] == producto_graf]
    X_test_prod = X_test[X_test["Commodity"] == producto_graf].drop(columns=["Date","Commodity"])

    y_true = invertir_transformaciones(
        y_test[y_test["Commodity"] == producto_graf]["resid_norm"].values,
        df_test_ref
    )
    y_pred = invertir_transformaciones(
        resultados[producto_graf]["modelo"].predict(X_test_prod),
        df_test_ref
    )

    residuos = y_true - y_pred


# Crear figura
plt.figure(figsize=(12,5))

# Precio real vs predicho
plt.subplot(1,2,1)
plt.plot(df_test_ref["Date"], y_true, label="Real", marker='o')
plt.plot(df_test_ref["Date"], y_pred, label="Predicho", marker='x')
plt.title(f"Precio real vs predicho - {producto_graf}")
plt.xlabel("Fecha")
plt.ylabel("Precio")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# Residuos
plt.subplot(1,2,2)
sns.histplot(residuos, bins=20, kde=True)
plt.title(f"Distribuci√≥n de residuos - {producto_graf}")
plt.xlabel("Residuo (Real - Predicho)")
plt.ylabel("Frecuencia")
plt.grid(True)

plt.tight_layout()
plt.show()

#Graficos:import matplotlib.pyplot as plt
import seaborn as sns

# Producto que queremos graficar
producto_graf = "Apple(Jholey)"

# Extraer resultados del test
df_test_ref = test[test["Commodity"] == producto_graf]
y_true = invertir_transformaciones(y_test[y_test["Commodity"] == producto_graf]["resid_norm"].values, df_test_ref)
y_pred = invertir_transformaciones(resultados[producto_graf]["modelo"].predict(
    X_test[X_test["Commodity"] == producto_graf].drop(columns=["Date", "Commodity"])
), df_test_ref)

# Crear figura
plt.figure(figsize=(12,5))

# 1Ô∏è‚É£ Precio real vs predicho
plt.subplot(1,2,1)
plt.plot(df_test_ref["Date"], y_true, label="Real", marker='o')
plt.plot(df_test_ref["Date"], y_pred, label="Predicho", marker='x')
plt.title(f"Precio real vs predicho - {producto_graf}")
plt.xlabel("Fecha")
plt.ylabel("Precio")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# 2Ô∏è‚É£ Residuos
plt.subplot(1,2,2)
residuos = y_true - y_pred
sns.histplot(residuos, bins=20, kde=True)
plt.title(f"Distribuci√≥n de residuos - {producto_graf}")
plt.xlabel("Residuo (Real - Predicho)")
plt.ylabel("Frecuencia")
plt.grid(True)

plt.tight_layout()
plt.show()

# Seleccionar los 10 productos con menor error en test
top_10 = df_resultados.sort_values("test_MAPE/sMAPE").head(10)["Producto"].tolist()

# Graficar cada uno
plt.figure(figsize=(6,3))
for producto in top_10:
    res = resultados[producto]
    modelo = res["modelo"]

    # Recuperar datos de test para ese producto
    data_p = df_feat[df_feat["Commodity"] == producto].copy().dropna()
    split_test = int(len(data_p) * 0.9)
    X_test = data_p.drop(columns=["Date", "Commodity", "Average", "Average_norm"]).iloc[split_test:]
    df_ref = data_p.iloc[split_test:]

    # Predicciones
    y_pred = modelo.predict(X_test)

    # Invertir transformaciones
    y_true = invertir_transformaciones(df_ref["Average_norm"], df_ref)
    y_pred_inv = invertir_transformaciones(y_pred, df_ref)

    # Gr√°fico
    plt.figure(figsize=(6,3))
    plt.plot(df_ref["Date"], y_true, label="Precio Real", color="black", linewidth=2)
    plt.plot(df_ref["Date"], y_pred_inv, label="Predicci√≥n XGBoost", color="tomato", linewidth=2, alpha=0.8)
    plt.title(f"Evoluci√≥n del Precio - {producto}\n(MAPE/sMAPE: {res['test_MAPE/sMAPE']:.2f}%)", fontsize=8)
    plt.xlabel("Fecha", fontsize=6)
    plt.ylabel("Precio Promedio", fontsize=6)
    plt.legend(fontsize=6)
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.show()

"""##2.3 ARIMA

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --force-reinstall pmdarima

"""Please run the ARIMA code cell (`hxjqKxX56bPe`) again after the installation is complete. If the `ValueError` persists, you might need to restart the Colab runtime (`Runtime > Restart runtime`) and then re-run the cells from the beginning (skipping the initial library installations if they were successful)."""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pmdarima

"""After the installation is complete, please run the cell with the ARIMA code again."""

# ================================
# 1. Seleccionar los 80 productos con m√°s datos
# ================================
conteo_productos = (
    df.groupby("Commodity")["Average"]
    .count()
    .reset_index(name="n_registros")
    .sort_values("n_registros", ascending=False)
)

top_80_productos = conteo_productos.head(80)["Commodity"].tolist()
df_top80 = df[df["Commodity"].isin(top_80_productos)].copy()
df_top80 = df_top80.sort_values(["Commodity", "Date"])

# ================================
# 2. Entrenar ARIMA por producto
# ================================
resultados_arima = {}

for producto in tqdm(top_80_productos):
    data_p = df_top80[df_top80["Commodity"] == producto].dropna()

    # Target
    y = data_p["Average"].values

    # Train-test split temporal (80/20)
    split = int(len(y) * 0.8)
    y_train, y_test = y[:split], y[split:]

    if len(y_train) < 50 or len(y_test) < 10:
        continue

    try:
        # AutoARIMA
        model = pm.auto_arima(
            y_train,
            seasonal=False,
            stepwise=True,
            suppress_warnings=True,
            error_action="ignore"
        )

        # Predicciones
        y_pred = model.predict(n_periods=len(y_test))

        # M√©tricas
        mape = mean_absolute_percentage_error(y_test, y_pred)
        rmse = math.sqrt(mean_squared_error(y_test, y_pred))

        resultados_arima[producto] = {
            "modelo": model,
            "mape": mape,
            "rmse": rmse,
            "y_test": y_test,
            "y_pred": y_pred
        }

    except Exception as e:
        print(f"Error con {producto}: {e}")
        continue

# ================================
# 3. Resumen de m√©tricas
# ================================
df_arima_metrics = pd.DataFrame({
    "Producto": list(resultados_arima.keys()),
    "MAPE": [v["mape"] for v in resultados_arima.values()],
    "RMSE": [v["rmse"] for v in resultados_arima.values()]
})

print(df_arima_metrics.sort_values("MAPE"))
print("MAPE promedio total ARIMA:", df_arima_metrics["MAPE"].mean())
print("RMSE promedio total ARIMA:", df_arima_metrics["RMSE"].mean())

# Top 10 productos con menor MAPE
top10_arima = df_arima_metrics.sort_values("MAPE").head(10)["Producto"].tolist()

for producto in top10_arima:
    plt.figure(figsize=(12, 5))
    y_test = resultados_arima[producto]["y_test"]
    y_pred = resultados_arima[producto]["y_pred"]

    plt.plot(y_test, label="Valores reales", marker="o")
    plt.plot(y_pred, label="Predicci√≥n ARIMA", marker="x")
    plt.title(f"Predicci√≥n ARIMA - {producto}")
    plt.xlabel("Tiempo (test set)")
    plt.ylabel("Precio promedio")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Top 10 productos con menor MAPE
top10_arima = df_arima_metrics.sort_values("MAPE").head(10)["Producto"].tolist()

for producto in top10_arima:
    plt.figure(figsize=(12, 5))
    y_test = resultados_arima[producto]["y_test"]
    y_pred = resultados_arima[producto]["y_pred"]

    plt.plot(y_test, label="Valores reales", marker="o")
    plt.plot(y_pred, label="Predicci√≥n ARIMA", marker="x")
    plt.title(f"Predicci√≥n ARIMA - {producto}")
    plt.xlabel("Tiempo (test set)")
    plt.ylabel("Precio promedio")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Crear DataFrames comparativos
df_rf_metrics = pd.DataFrame({
    "Producto": list(resultados_rf.keys()),
    "MAPE_RF": [v["mape"] for v in resultados_rf.values()],
    "RMSE_RF": [v["rmse"] for v in resultados_rf.values()]
})

df_xgb_metrics = pd.DataFrame({
    "Producto": list(resultados_xgb.keys()),
    "MAPE_XGB": [v["mape"] for v in resultados_xgb.values()],
    "RMSE_XGB": [v["rmse"] for v in resultados_xgb.values()]
})

# Unir todo
df_comparacion = df_arima_metrics.merge(df_rf_metrics, on="Producto").merge(df_xgb_metrics, on="Producto")

print(df_comparacion.head())

# Gr√°fico comparativo MAPE promedio
mape_promedios = {
    "ARIMA": df_arima_metrics["MAPE"].mean(),
    "Random Forest": df_rf_metrics["MAPE_RF"].mean(),
    "XGBoost": df_xgb_metrics["MAPE_XGB"].mean()
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(mape_promedios.keys()), y=list(mape_promedios.values()), palette="viridis")
plt.title("Comparaci√≥n de MAPE promedio entre modelos")
plt.ylabel("MAPE promedio")
plt.show()

# Gr√°fico comparativo RMSE promedio
rmse_promedios = {
    "ARIMA": df_arima_metrics["RMSE"].mean(),
    "Random Forest": df_rf_metrics["RMSE_RF"].mean(),
    "XGBoost": df_xgb_metrics["RMSE_XGB"].mean()
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(rmse_promedios.keys()), y=list(rmse_promedios.values()), palette="magma")
plt.title("Comparaci√≥n de RMSE promedio entre modelos")
plt.ylabel("RMSE promedio")
plt.show()

"""#3. PRODUCTO
1. Selecci√≥n del modelo
2. Confecci√≥n del producto
3. ¬øAn√°lisis de impacto esperado?
"""