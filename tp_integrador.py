# -*- coding: utf-8 -*-
"""TP Integrador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w4B5r6xWCQw_hxGfO57H8YF-f_AULdrv

1. EDA
   1. Variables: selección, unificación y agregado de 'Type'
   2. Productos: descarte según calidad de la serie
   3. Outliers: identificación
   4. Transformación para modelado: descomposición, normalización y estructuración secuencial
2. MODELOS
   1. XGBoost Global: entrenamiento, validación por expanding window y evaluación
   2. XGBoost por producto
   3. ARIMA
3. PRODUCTO
   1. Selección del modelo
   2. Confección del producto
   3. ¿Análisis de impacto esperado?
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

# Commented out IPython magic to ensure Python compatibility.
#Importación de Librerias
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Configuraciones para visualización
# %matplotlib inline
sns.set(style="whitegrid")

#...Agregar librerias a medida que sean necesarias

#Importación de datos desde Kaggle
import kagglehub
from kagglehub import KaggleDatasetAdapter

#Omisión de warnings
import warnings
warnings.filterwarnings('ignore')
from statsmodels.tools.sm_exceptions import InterpolationWarning
warnings.simplefilter('ignore', InterpolationWarning)

#Modelado
import xgboost as xgb
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score
from statsmodels.tsa.seasonal import STL # Season-Trend decomposition using LOESS.
from statsmodels.tsa.stattools import adfuller, kpss  # ADF test (Augmented Dickey-Fuller) para no-estacionariedad y
                                                      # KPSS test (Kwiatkowski-Phillips-Schmidt-Shin) para estacionariedad. Complementarios
import joblib
#import pmdarima as pm #problemas de versiones con otras librerias (numpy)
from tqdm import tqdm

#Importacion de Dataset
dataset_id = "ramkrijal/agriculture-vegetables-fruits-time-series-prices"
csv_file = "kalimati_tarkari_dataset.csv"

try:
    df_vege_fruits = kagglehub.load_dataset(
        KaggleDatasetAdapter.PANDAS,
        dataset_id,
        csv_file
    )
    print("Dataset cargado correctamente.")
except Exception as e:
    print("Error al cargar el dataset:", e)

"""#1. EDA
1. Variables
2. Productos
3. Outliers
4. Transformación para modelado

##1.1 VARIABLES
"""

df_vege_fruits.head()

df_vege_fruits.info()

# Convertimos la columna Date a formato datetime
df_vege_fruits['Date'] = pd.to_datetime(df_vege_fruits['Date'])

# Revisamos los tipos nuevamente
df_vege_fruits.dtypes

# Eliminar columnas que no usaremos: número de serie porque no aporta, mínimo y máximo porque vamos a restringir la complejidad del análisis.
df_vege_fruits.drop(columns=['SN','Minimum','Maximum'], inplace=True)

df_vege_fruits.describe(include='all')

# Conteo de valores nulos por columna
df_vege_fruits.isnull().sum()

# Duplicados
df_vege_fruits.duplicated().sum()

# Unidades
df_vege_fruits['Unit'].unique()

# Unificamos KG y Kg porque son la misma unidad
df_vege_fruits['Unit'] = df_vege_fruits['Unit'].replace({'KG':'Kg'})

# Productos
df_vege_fruits['Commodity'].unique()

df_vege_fruits['Commodity'].nunique()

# Separamos en categorías de productos

verduras = [
    "Tomato Big(Nepali)", "Tomato Small(Local)", "Tomato Small(Tunnel)",
    "Tomato Big(Indian)", "Tomato Small(Indian)", "Tomato Small(Terai)",
    "Potato Red", "Potato White", "Potato Red(Indian)", "Potato Red(Mude)",
    "Onion Dry (Indian)", "Onion Dry (Chinese)",
    "Carrot(Local)", "Carrot(Terai)",
    "Cabbage(Local)", "Cabbage(Terai)", "Cabbage", "Red Cabbbage",
    "Cauli Local", "Cauli Local(Jyapu)", "Cauli Terai",
    "Raddish Red", "Raddish White(Local)", "Raddish White(Hybrid)",
    "Brinjal Long", "Brinjal Round",
    "Cow pea(Long)", "Cowpea(Short)",
    "Green Peas",
    "French Bean(Local)", "French Bean(Hybrid)", "French Bean(Rajma)",
    "Soyabean Green",
    "Bitter Gourd", "Bottle Gourd", "Pointed Gourd(Local)", "Pointed Gourd(Terai)",
    "Snake Gourd", "Smooth Gourd", "Sponge Gourd", "Pumpkin",
    "Squash(Long)", "Squash(Round)",
    "Turnip", "Turnip A",
    "Okara", "Christophine",
    "Brd Leaf Mustard", "Mustard Leaf", "Spinach Leaf", "Cress Leaf", "Fenugreek Leaf",
    "Onion Green",
    "Ginger",
    "Chilli Dry", "Chilli Green", "Chilli Green(Bullet)", "Chilli Green(Machhe)", "Chilli Green(Akbare)",
    "Capsicum",
    "Garlic Green", "Garlic Dry Chinese", "Garlic Dry Nepali",
    "Clive Dry", "Clive Green", "Coriander Green",
    "Asparagus", "Neuro", "Brocauli", "Sugarbeet", "Drumstick",
    "Lettuce", "Celery", "Parseley", "Fennel Leaf", "Mint",
    "Bamboo Shoot", "Arum", "Maize", "Sword Bean",
    "Yam", "Sweet Potato", "Knolkhol",
    "Cucumber(Local)", "Cucumber(Hybrid)", "Bauhania flower"
]

frutas = [
    "Apple(Jholey)", "Apple(Fuji)",
    "Banana",
    "Lime", "Lemon", "Sweet Lime",
    "Orange(Nepali)", "Orange(Indian)", "Sweet Orange", "Mandarin", "Kinnow",
    "Grapes(Green)", "Grapes(Black)",
    "Pomegranate",
    "Mango(Maldah)", "Mango(Dushari)", "Mango(Calcutte)", "Mango(Chousa)",
    "Water Melon(Green)", "Water Melon(Dotted)", "Musk Melon",
    "Jack Fruit",
    "Papaya(Nepali)", "Papaya(Indian)",
    "Sugarcane",
    "Pineapple",
    "Guava", "Mombin", "Barela", "Bakula",
    "Pear(Local)", "Pear(Chinese)",
    "Litchi(Local)", "Litchi(Indian)",
    "Kiwi", "Strawberry"
]

otros = ["Tofu", "Gundruk", "Tamarind", "Mushroom(Kanya)",
         "Mushroom(Button)", "Fish Fresh", "Fish Fresh(Rahu)",
         "Fish Fresh(Bachuwa)", "Fish Fresh(Chhadi)", "Fish Fresh(Mungari)"]

print('Verduras: ',len(verduras))
print('Frutas: ',len(frutas))
print('Otros: ',len(otros))
print('Total: ',len(verduras)+len(frutas)+len(otros))

# Definimos la función de clasificación
def clasificar_producto(prod):
    if prod in verduras:
        return "Vegetable"
    elif prod in frutas:
        return "Fruit"
    elif prod in otros:
        return "Other"
    else:
        return "Unknown"

# Creamos una nueva columna para la categoría
df_vege_fruits["Type"] = df_vege_fruits["Commodity"].apply(clasificar_producto)

df_vege_fruits.info()

df_vege_fruits.head()

"""##1.1 PRODUCTOS"""

#Cantidad de datos por producto
pd.set_option('display.max_rows', None)
display(df_vege_fruits['Commodity'].value_counts())
pd.reset_option('display.max_rows')

"""Quitamos los que tienen menos de 200 entradas"""

N_min = 200

# Obtener la cantidad de datos por producto
commodity_counts = df_vege_fruits['Commodity'].value_counts()

# Separar los productos con más de N_min registros
commodities_more_than_N_min = commodity_counts[commodity_counts > N_min].index.tolist()

# Separar los productos con N_min registros o menos
commodities_N_min_or_less = commodity_counts[commodity_counts <= N_min].index.tolist()

print(f"Productos con más de {N_min} registros:")
display(commodities_more_than_N_min)

print(f"\nProductos con {N_min} registros o menos:")
display(commodities_N_min_or_less)

num_commodities_more_than_N_min = len(commodities_more_than_N_min)
print(f"Hay {num_commodities_more_than_N_min} productos con más de {N_min} registros.")

df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_N_min_or_less)]
df_vege_fruits['Commodity'].nunique()

"""Revisamos la distribución de días entre entradas para identificar anomalías"""

# ordenamos por producto y fecha
df_vege_fruits = df_vege_fruits.sort_values(["Commodity", "Date"])

# diferencias entre fechas dentro de cada producto
df_aux = df_vege_fruits.copy()
df_aux["delta_dias"] = (
    df_vege_fruits.groupby("Commodity")["Date"].diff().dt.days  #total_seconds() / (24*3600)
)

# intervalo medio por producto
intervalo_medio = df_aux.groupby("Commodity")["delta_dias"].mean()

"""Quitamos los que tienen una frecuencia promedio menor a una entrada por semana"""

# productos válidos: intervalo medio <= 7 días
productos_validos = intervalo_medio[intervalo_medio <= 7].index

# filtramos el dataframe original
df_vege_fruits = df_vege_fruits[df_vege_fruits["Commodity"].isin(productos_validos)]
df_vege_fruits['Commodity'].nunique()

df_aux['delta_dias'].describe()
# valor máximo 2054 y desvío 11.9!!!

#intervalo_medio.head()
df_aux.sort_values(by='delta_dias', ascending=False)#.head(10)

df_aux.isnull().sum()
# está bien, parecen ser el principio y final de la serie porque hay
# uno por producto (al calcular las diferencias queda con una entrada menos)

# Distribución de intervalos global para intervalos grandes

lower_limit = 30
tmp_df = df_aux[df_aux['delta_dias'] > lower_limit]
#x = np.linspace(0, 2100, 11)
plt.figure(figsize=(14, 4))
plt.hist(tmp_df['delta_dias'],bins=300)
plt.title("Acumulado de todos los productos")
plt.xlabel(f"Diferencia de días entre entradas (>{lower_limit})")
#plt.xticks(x)
plt.tight_layout()
plt.show()

"""Quitamos los productos que tienen periodos faltantes más largos que un fuera de temporada estimado"""

# asumiendo al menos 2 meses de temporada, fuera de temporada: 10x30 = 300
tmp_df = df_aux[df_aux['delta_dias'] > 300]
tmp_df.describe(include='all')

commodities_big_gap = tmp_df['Commodity'].unique()
commodities_big_gap

# Revisamos las curvas
for commodity in commodities_big_gap:
    plt.figure(figsize=(14, 4))
    temp_df = df_vege_fruits[df_vege_fruits['Commodity'] == commodity]
    plt.scatter(temp_df['Date'], temp_df['Average'], label=commodity, marker='.', s=5)
    plt.legend()
    #plt.title("Evolución de precios promedios")
    plt.xlabel("Fecha")
    plt.ylabel("Precio promedio por unidad de medida")
    plt.tight_layout()
    plt.show()

# Quitamos esos productos por tener series muy irregulares o con pocos datos hacia el final
df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_big_gap)]

# actualizamos intervalo medio por producto
df_aux = df_aux[~df_aux["Commodity"].isin(commodities_big_gap)]
intervalo_medio = df_aux.groupby("Commodity")["delta_dias"].mean()

df_vege_fruits['Commodity'].nunique()

# Volvemos a revisar la distribución de intervalos total

#lower_limit = 5
tmp_df = df_aux[df_aux['delta_dias'] > lower_limit]
#x = np.linspace(0, 2100, 11)
plt.figure(figsize=(14, 4))
plt.hist(tmp_df['delta_dias'],bins=300)
plt.title("Acumulado de todos los productos")
plt.xlabel(f"Diferencia de días entre entradas (>{lower_limit})")
#plt.xticks(x)
plt.tight_layout()
plt.show()

"""Identificamos los que tienen una frecuencia promedio menor a una entrada cada tres días"""

# productos a revisar: 3 días < intervalo medio <= 7 días
productos_a_revisar = intervalo_medio[(3 < intervalo_medio) & (intervalo_medio <= 7)].index

# días promedio entre entradas
intervalo_medio[productos_a_revisar]

# Las curvas no parecen mal así que no los quitamos

plt.figure(figsize=(14, 5))
for commodity in productos_a_revisar:
    temp_df = df_vege_fruits[df_vege_fruits['Commodity'] == commodity]
    plt.scatter(temp_df['Date'], temp_df['Average'], label=commodity, marker='.', s=5)

plt.legend()
plt.title("Evolución de precios promedios")
plt.xlabel("Fecha")
plt.ylabel("Precio promedio por unidad de medida")
plt.tight_layout()
plt.show()

"""Revisamos la cantidad de entradas que tiene cada producto en el último año"""

N_days = 365

# calcular fecha de corte
fecha_max = df_vege_fruits["Date"].max()
fecha_corte = fecha_max - pd.Timedelta(days=N_days)

# filtrar últimos N_days días
df_ultimos_N_days = df_vege_fruits[df_vege_fruits["Date"] >= fecha_corte]

# contar entradas por producto
conteo_por_producto = df_ultimos_N_days.groupby("Commodity").size().reset_index(name="conteo")

pd.set_option('display.max_rows', None)
display(conteo_por_producto)
pd.reset_option('display.max_rows')

"""Quitamos los que tienen menos de 1 dato cada 3 días en el último año, dos años, tres años y cuatro años."""

days_in_year = 365

for i in [1,2,3,4]:
  # calcular fecha de corte
  N_days = i*days_in_year
  fecha_corte = fecha_max - pd.Timedelta(days=N_days)

  # filtrar últimos N_days días
  df_ultimos_N_days = df_vege_fruits[df_vege_fruits["Date"] >= fecha_corte]

  # contar entradas por producto
  conteo_por_producto = df_ultimos_N_days.groupby("Commodity").size().reset_index(name="conteo")

  # quitar los productos con pocas entradas el último periodo
  min_entries = 120*i
  commodities_no_resent_data = conteo_por_producto[conteo_por_producto['conteo'] < min_entries]['Commodity']
  df_vege_fruits = df_vege_fruits[~df_vege_fruits["Commodity"].isin(commodities_no_resent_data)]

df_vege_fruits['Commodity'].nunique()

# Volvemos a revisar la cantidad de datos por producto
pd.set_option('display.max_rows', None)
display(df_vege_fruits['Commodity'].value_counts())
pd.reset_option('display.max_rows')

"""##1.3 OUTLIERS
Exploramos los outliers según IQR. Como los modelos que vamos a usar son sensibles frente outliers (ARIMA más que nada) usaremos STL, que cuenta con un método robusto ante outliers, para descomponer las series previamente. Los dejamos por el momento y retomamos para los productos en los que sea necesario.
"""

# Estadística descriptiva por producto
variables = ['Average'] #['Minimum', 'Maximum', 'Average']
df_vege_fruits.groupby('Commodity')[variables].describe()

# gráfico interactivo con dropdown de producto
fig = px.scatter(
    df_vege_fruits,
    x="Date",
    y="Average",
    color="Commodity",            # esto permite tener varios productos
    title="Evolución de precios por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evolución de precios — {prod}"}
                    ],
                }
                for prod in df_vege_fruits["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

# Ver cuántas combinaciones hay de Producto + Tipo + Unidad
df_vege_fruits.groupby(['Commodity', 'Type', 'Unit']).size().reset_index(name='count').head(20)

cant_verduras = 20 # Se mostraran solo esta cantidad para que el grafico sea legible

df_veg = df_vege_fruits[df_vege_fruits['Type'] == 'Vegetable']
top_veg = df_veg['Commodity'].value_counts().nlargest(cant_verduras).index

df_top_veg = df_veg[df_veg['Commodity'].isin(top_veg)]
#df_top_veg = df_veg   #Descomentar para mostrar Todas las verduras

fig = px.box(
    df_top_veg,
    x="Average",
    y="Commodity",
    color="Commodity",
    title="Distribución de Precios de Verduras",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Verduras"}
)
fig.show()

df_fruit = df_vege_fruits[
    (df_vege_fruits['Type'] == 'Fruit') & (df_vege_fruits['Unit'] == 'Kg')
]

fig = px.box(
    df_fruit,
    x="Average",
    y="Commodity",
    facet_col="Unit",   # panel por unidad de medida
    color="Commodity",
    title="Distribución de Precios de Frutas",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Frutas"}
)
fig.show()

df_fruit = df_vege_fruits[
(df_vege_fruits['Type'] == 'Fruit') & (df_vege_fruits['Unit'] != 'Kg')]

fig = px.box(
    df_fruit,
    x="Average",
    y="Commodity",
    facet_col="Unit",   # panel por unidad de medida
    color="Commodity",
    title="Distribución de Precios de Frutas",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Frutas"}
)
fig.show()

df_other = df_vege_fruits[df_vege_fruits['Type'] == 'Other']

fig = px.box(
    df_other,
    x="Average",
    y="Commodity",
    color="Commodity",
    title="Distribución de Precios Otros ",
    labels={"Average": "Precio Promedio", "Commodity": "Producto - Otros"}
)
fig.show()

# Identificación de OUTLIERS con IQR
def identify_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filtrar outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Identificación de OUTLIERS con Z-Score
def identify_outliers_zscore(df,column):
    # función
    return 0

# Identificación de OUTLIERS con IQR diferenciando por producto
outliers_by_comm = {}
columns = ['Average'] # ['Minimum', 'Maximum', 'Average']

for product in df_vege_fruits['Commodity'].unique():
    df_comm = df_vege_fruits[df_vege_fruits['Commodity'] == product]
    for col in columns:
        outliers_df = identify_outliers_iqr(df_comm, col)
        if not outliers_df.empty:
            outliers_by_comm[(product, col)] = outliers_df
            print(f"Outliers detectados en '{col}' para '{product}': {outliers_df.shape[0]}")
            # Opcional: mostrar algunos ejemplos
            #print(outliers_df[['Commodity','Type', 'Date', col]].head())

"""##1.4 TRANSFORMACIÓN PARA MODELADO

Los modelos que vamos a usar sólo sirven para series estacionarias sin estacionalidad. Hay que quitarles las tendencias y la estacionalidad a las series previamente.

* XGBoost: para que pueda tomar series temporales hay que estructurar los datos.
* ARIMA: también se podría probar usar SARIMA para las series que tengan fuerte estacionalidad (sin quitarla previamente).

###Pruebas de estacionareidad ADF y KPSS:

ADF:   p < 0.05 → estacionaria.

KPSS:  p < 0.05 → NO estacionaria.

Para el mismo criterio de valor de p (0.05):

*   Caso 1: Ambos tests concluyen que la serie no es estacionaria → La serie no es estacionaria.

*   Caso 2: Ambos tests concluyen que la serie es estacionaria → La serie es estacionaria.

*   Caso 3: El test KPSS indica estacionariedad y el test ADF indica no estacionariedad → La serie es estacionaria en tendencia (trend stationary).
Debe eliminarse la tendencia para que la serie sea estrictamente estacionaria.
Luego se verifica la estacionariedad de la serie detrendida.

*   Caso 4: El test KPSS indica no estacionariedad y el test ADF indica estacionariedad → La serie es estacionaria en diferencias (difference stationary).
Debe aplicarse diferenciación para hacer la serie estacionaria.
Después se verifica la estacionariedad de la serie diferenciada.
"""

# Función para descomponer una serie: original = tendencia + estacional + residuo
def seasonality_LOESS(serie, period=None, robust=False):
    stl = STL(serie.dropna(), period=period)
    decomposition = stl.fit()

    var_resid = np.var(decomposition.resid)
    var_seasonal_resid = np.var(decomposition.resid + decomposition.seasonal)
    strength = max(0, 1 - var_resid/var_seasonal_resid)

    return decomposition, strength

# Cargar los datos
df = df_vege_fruits.copy()
productos_seleccionados = df['Commodity'].unique().tolist()

# Analizar tendencia y estacionalidad por producto
comportamiento = {}
for producto in productos_seleccionados:
    serie_producto = df[df['Commodity']==producto].set_index("Date")["Average"]

    # Descomposición tendencia+estacionalidad+residuo
    try:
        decomposition, strength = seasonality_LOESS(serie_producto, robust=True)
        print(f'STL auto-period for {producto}')
    except:
        decomposition, strength = seasonality_LOESS(serie_producto, robust=True, period=365)
        #print(f'STL period 365d for {producto}')

    # Estacionariedad antes y después de descomponer
    estacionariedad_ADF_prev_STL = adfuller(serie_producto.dropna())
    estacionariedad_ADF_post_STL = adfuller(decomposition.resid)
    estacionariedad_KPSS_prev_STL = kpss(serie_producto.dropna())
    estacionariedad_KPSS_post_STL = kpss(decomposition.resid)

    # Resultados
    comportamiento[producto] = {
        'decomposition': decomposition,
        'strength': strength,
        'adf_pvalue_before_stl': estacionariedad_ADF_prev_STL[1],
        'adf_pvalue_after_stl': estacionariedad_ADF_post_STL[1],
        'kpss_pvalue_before_stl': estacionariedad_KPSS_prev_STL[1],
        'kpss_pvalue_after_stl': estacionariedad_KPSS_post_STL[1]
    }

# Pasar el dict a DataFrame
df_comportamiento = pd.DataFrame.from_dict(comportamiento, orient="index")

# Resetear índice para que 'Commodity' quede como columna
df_comportamiento = df_comportamiento.reset_index().rename(columns={"index": "Commodity"})

# Filtrar solo las columnas de interés
df_comportamiento = df_comportamiento[[
    "Commodity",
    "strength",
    "adf_pvalue_before_stl",
    "adf_pvalue_after_stl",
    "kpss_pvalue_before_stl",
    "kpss_pvalue_after_stl"
]]

# Estacionalidad
# strength ~ 1: fuerte estacionalidad
full_print=False
plot=False

# Identificar productos con estacionalidad
# El valor de strength fue elegido arbitrariamente: buscar algún criterio!
series_sin_estacionalidad = df_comportamiento[df_comportamiento['strength'] < 0.5]
series_con_estacionalidad = df_comportamiento[df_comportamiento['strength'] >= 0.5]
N_series_with_seas = series_con_estacionalidad['Commodity'].value_counts().sum()

print(f'Cantidad de series CON estacionalidad = {N_series_with_seas}')
if full_print:
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print(series_con_estacionalidad.sort_values(by='strength', ascending=False))
else:
    print(series_con_estacionalidad.sort_values(by='strength', ascending=False))

# Gráficos de las componentes de la serie (original=trend+seasonal+residual)
if plot:
    for producto in productos_seleccionados:
        decomposition = comportamiento[producto]['decomposition']
        strength = comportamiento[producto]['strength']
        adf_pvalue_before = comportamiento[producto]['avf_pvalue_before_stl']
        adf_pvalue_after = comportamiento[producto]['avf_pvalue_after_stl']
        kpss_pvalue_before = comportamiento[producto]['kpss_pvalue_before_stl']
        kpss_pvalue_after = comportamiento[producto]['kpss_pvalue_after_stl']

        print(f'\n{producto}: strength = {strength}')
        print(f'adf-p-before = {adf_pvalue_before}\t adf-p-after = {adf_pvalue_after}\t kpss-p-before = {kpss_pvalue_before}\t kpss-p-after = {kpss_pvalue_after}')
        decomposition.plot()
        plt.show()

# Estacionariedad
# ADF   p < 0.05 → estacionaria.    p < 0.025 más conservador (más probabilidad de requerir transformación)
# KPSS  p < 0.05 → NO estacionaria. p < 0.075 más conservador (más probabilidad de requerir transformación)
full_print=False

# Identificar series NO estacionarias ANTES de la descomposición
series_adf_no_estacionarias_antes = df_comportamiento[df_comportamiento['adf_pvalue_before_stl'] >= 0.05]
N_series_adf_non_stat_before = series_adf_no_estacionarias_antes['Commodity'].value_counts().sum()
series_kpss_no_estacionarias_antes = df_comportamiento[df_comportamiento['kpss_pvalue_before_stl'] < 0.05]
N_series_kpss_non_stat_before = series_kpss_no_estacionarias_antes['Commodity'].value_counts().sum()

# Identificar series NO estacionarias DESPUÉS de la descomposición
series_adf_no_estacionarias_despues = df_comportamiento[df_comportamiento['adf_pvalue_after_stl'] >= 0.05]
N_series_adf_non_stat_after = series_adf_no_estacionarias_despues['Commodity'].value_counts().sum()
series_kpss_no_estacionarias_despues = df_comportamiento[df_comportamiento['kpss_pvalue_after_stl'] < 0.05]
N_series_kpss_non_stat_after = series_adf_no_estacionarias_despues['Commodity'].value_counts().sum()

# Imprimir resultados
print(f'Series NO estacionarias pre-STL = {N_series_adf_non_stat_before} (ADF)\t {N_series_kpss_non_stat_before} (KPSS)')
print(f'Series NO estacionarias pos-STL = {N_series_adf_non_stat_after} (ADF)\t {N_series_adf_non_stat_after} (KPSS)')
if full_print:
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        print(series_adf_no_estacionarias_antes.sort_values(by='adf_pvalue_before_stl', ascending=False))
        print(series_kpss_no_estacionarias_antes.sort_values(by='kpss_pvalue_before_stl', ascending=False))
else:
    print(series_adf_no_estacionarias_antes.sort_values(by='adf_pvalue_before_stl', ascending=False))
    print(series_kpss_no_estacionarias_antes.sort_values(by='kpss_pvalue_before_stl', ascending=False))

"""Acá habría que transformar las series si hiciera falta hacerlas estacionarias luego de quitarles la tendencia y estacionalidad. Parece que no va a ser necesario porque todas las series transformadas (resid) son estacionarias a nivel de significancia 0.05.

Nota: KPSS sólo puede reportar p-valores menores a 0.1, mientras usemos nivel de significancia menor a eso estamos bien.

Ahora agregamos las tres descomposiciones al dataframe.
"""

rows = []
for producto in productos_seleccionados:
    serie_producto = df[df['Commodity']==producto].set_index("Date")["Average"]
    seasonal = comportamiento[producto]['decomposition'].seasonal
    trend = comportamiento[producto]['decomposition'].trend
    resid = comportamiento[producto]['decomposition'].resid

    tmp = pd.DataFrame({
        "Date": serie_producto.index,
        "Commodity": producto,
        "seasonal": seasonal.values,
        "trend": trend.values,
        "resid": resid.values
    })
    rows.append(tmp.reset_index(drop=True))

temp_df = pd.concat(rows, ignore_index=True)
temp_df.info()

print(df[["Commodity","Date"]].duplicated().sum())        # debería ser 0
print(temp_df[["Commodity","Date"]].duplicated().sum())   # debería ser 0

df_full = df.merge(
    temp_df,
    on=["Commodity", "Date"],
    how="left" #"inner", usar "left" si querés conservar todas las filas de df
)

df_full.info()

# gráfico interactivo con dropdown de producto
fig = px.scatter(
    df_full,
    x="Date",
    y="resid",
    color="Commodity",            # esto permite tener varios productos
    title="Evolución de residuos por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evolución de residuos — {prod}"}
                    ],
                }
                for prod in df_full["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

# Función que adapta la base de datos para poder usar modelos de ML no
# específicos de series temporales
def estructurar_datos(data, target, lags=[1, 2, 3, 7, 14, 30], low_cat_feat=None):
    df_feat = data.copy()

    # El modelo no maneja fechas, derivamos variables numéricas
    # para mayor sensibilidad a estacionalidades
    df_feat["dayofyear"] = df_feat["Date"].dt.dayofyear
    df_feat["dayofweek"] = df_feat["Date"].dt.dayofweek
    df_feat["month"] = df_feat["Date"].dt.month
    df_feat["year"] = df_feat["Date"].dt.year

    # One-hot encoding para categóricas de poco rango que potencialmente
    # puedan aportar al entrenamiento
    if low_cat_feat is not None:
        for ft in low_cat_feat:
            df_feat = pd.get_dummies(df_feat, columns=[ft], drop_first=True)
            print(f'\'{ft}\' one-hot encoded')

    # Target encoding para productos así los tiene en cuenta
    mean_prices = df_feat.groupby("Commodity")[target].mean()
    df_feat["Commodity_t_encoded"] = df_feat["Commodity"].map(mean_prices)
    print('\'Commodity\' mean-encoded into \'Commodity_t_encoded\'')

    # Normalización para estabilizar escalas
    stats = df_feat.groupby("Commodity")[target].agg(["mean", "std"])
    df_feat = df_feat.merge(stats, on="Commodity")
    df_feat[target+"_norm"] = (df_feat[target] - df_feat["mean"]) / df_feat["std"]
    print(f'\'{target}\' normalized into \'{target}_norm\'')

    # Se agregan variables con corrimiento y medias móviles
    # que le dan estructura secuencial a los datos.
    for lag in lags:
        df_feat[f"lag_{lag}"] = df_feat.groupby("Commodity")[target+"_norm"].shift(lag)
        print(f'Added lag_{lag}')

        df_feat[f"roll_mean_{lag}"] = (
            df_feat.groupby("Commodity")[target+"_norm"]
            .shift(1)
            .rolling(window=lag)
            .mean()
        )
        print(f'Added roll_mean_{lag}')

    df_feat = df_feat.dropna()

    return df_feat

# Crear las features
df_feat = estructurar_datos(
    df_full,
    'resid',
    low_cat_feat=['Unit','Type']
)

df_feat.info()

# gráfico interactivo con dropdown de producto
fig = px.scatter(
    df_feat,
    x="Date",
    y="resid_norm",
    color="Commodity",            # esto permite tener varios productos
    title="Evolución de residuos normalizados por producto"
)

# para muchos productos mejor usar dropdown:
fig.update_layout(
    updatemenus=[
        {
            "buttons": [
                {
                    "method": "update",
                    "label": prod,
                    "args": [
                        {"visible": [p == prod for p in df_vege_fruits["Commodity"].unique()]},
                        {"title": f"Evolución de residuos normalizados — {prod}"}
                    ],
                }
                for prod in df_full["Commodity"].unique()
            ],
            "direction": "down",
        }
    ]
)

fig.show()

"""#2. MODELOS
1. XGBoost Global
2. XGBoost por producto
3. ARIMA

Consideraciones:
* Hacer entrenamiento y validación por expanding window con X_train_valid / y_train_valid.

* Evaluación con X_test / y_test.

* Siempre invertir las transformaciones antes de calcular métricas interpretables (RMSE, MAE, MAPE, R²): desnormalizar usando mean y std (df_feat) y agregarles tendencia y estacionalidad (trend y seasonal en df_feat) . Para series con ceros o valores muy pequeños, usar sMAPE en lugar de MAPE. Combinar métricas es lo ideal:

  * RMSE → magnitud del error
  * MAPE/sMAPE → error relativo
  * MFE → sesgo del modelo

Opcional: En la validación y evaluación se puede usar 'Average' como y_true para el cálculo de los errores y así alcanza con sólo invertir las transformaciones de y_pred. Se tomaría de df_feat y haría falta repetir los splits.
"""

# Separar en entrenamiento+validación y evaluación (últimos 12 meses)
fecha_corte = df_feat["Date"].max() - pd.DateOffset(months=12)

train_valid = df_feat[df_feat["Date"] < fecha_corte]
test = df_feat[df_feat["Date"] >= fecha_corte]

# Separar variables explicativas de variable objetivo
X_train_valid = train_valid.drop(columns=["Average","resid","resid_norm","trend","seasonal","mean","std"])
y_train_valid = train_valid[["Commodity", "resid_norm"]]

X_test = test.drop(columns=["Average","resid","resid_norm","trend","seasonal","mean","std"])
y_test = test[["Commodity","resid_norm"]]

# Funciones auxiliares
def f_smape(y_true, y_pred):
    """Symmetric Mean Absolute Percentage Error"""
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-8)) * 100

def f_mfe(y_true, y_pred):
    """Mean Forecast Error (sesgo)"""
    return np.mean(y_pred - y_true)

def invertir_transformaciones(y_pred, df_ref):
    """Invertir desnormalización y sumar tendencia + estacionalidad"""
    y_pred_1d = np.array(y_pred).ravel()  # Asegura vector 1D
    y_inv = (y_pred_1d * df_ref["std"].values) + df_ref["mean"].values
    y_inv += df_ref["trend"].values + df_ref["seasonal"].values
    return y_inv

def mape_condicional(y_true, y_pred, umbral_min=1.0):
    """
    Calcula MAPE o sMAPE según el umbral mínimo de valor real.
    Si algún valor < umbral_min -> sMAPE, sino MAPE
    """
    if np.any(y_true < umbral_min):
        return f_smape(y_true, y_pred)  # sMAPE
    else:
        return mean_absolute_percentage_error(y_true, y_pred) # np.mean(np.abs((y_pred - y_true) / y_true)) * 100  # MAPE

"""##2.1 XGBoost Global"""

# OPTIMIZACIÓN DE HIPERPARÁMETROS (Optuna + TimeSeriesSplit)
X = X_train_valid.drop(columns=["Commodity","Date"])
y = y_train_valid.drop(columns=["Commodity"])

N_FOLDS = 7
N_TRIALS = 30

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 700),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "subsample": trial.suggest_float("subsample", 0.7, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "random_state": 42,
        "n_jobs": -1,
        "objective": "reg:squarederror",
        "eval_metric": "rmse"
    }

    tscv = TimeSeriesSplit(n_splits=N_FOLDS)
    scores = []

    for train_idx, valid_idx in tscv.split(X):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)

        y_pred = model.predict(X_valid)
        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
        scores.append(rmse)

    return np.mean(scores)

print("🔎 Buscando hiperparámetros óptimos con Optuna...")
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=N_TRIALS)

best_params = study.best_params
print(f"\n🏆 Mejores parámetros: {best_params}")
print(f"✅ RMSE medio en validación: {study.best_value:.4f}")

# ===============================================================
# ENTRENAMIENTO FINAL CON LOS MEJORES PARÁMETROS
# ===============================================================
model_final = xgb.XGBRegressor(**best_params)
model_final.fit(X, y)

# Guardar modelo global
joblib.dump(model_final, "xgb_model_global_opt.pkl")

# ===============================================================
# EVALUACIÓN EN TEST
# ===============================================================
X_test_model = X_test.drop(columns=["Commodity","Date"])
y_pred_test = model_final.predict(X_test_model)

# Invertir transformaciones a escala original
y_pred_inv = invertir_transformaciones(y_pred_test, test)
y_true_inv = invertir_transformaciones(y_test.drop(columns="Commodity"), test)

# ===============================================================
# MÉTRICAS GLOBALES
# ===============================================================
rmse_global = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))
mape_global = mape_condicional(y_true_inv, y_pred_inv)
r2_global = r2_score(y_true_inv, y_pred_inv)
mfe_global = f_mfe(y_true_inv, y_pred_inv)

print("\n===== Desempeño Global en Test =====")
print(f"RMSE: {rmse_global:.3f}")
print(f"MAPE/sMAPE: {mape_global:.3f}")
print(f"R²: {r2_global:.3f}")
print(f"MFE: {mfe_global:.3f}")

# ===============================================================
# MÉTRICAS POR PRODUCTO
# ===============================================================
df_eval = test.copy()
df_eval["y_true_inv"] = y_true_inv
df_eval["y_pred_inv"] = y_pred_inv

metricas_prod = []
for prod, grp in df_eval.groupby("Commodity"):
    rmse = np.sqrt(mean_squared_error(grp["y_true_inv"], grp["y_pred_inv"]))
    mape = mape_condicional(grp["y_true_inv"], grp["y_pred_inv"])
    r2 = r2_score(grp["y_true_inv"], grp["y_pred_inv"])
    mfe = f_mfe(grp["y_true_inv"], grp["y_pred_inv"])
    metricas_prod.append({"Commodity": prod, "RMSE": rmse, "MAPE/sMAPE": mape, "R2": r2, "MFE": mfe})

df_metricas_prod = pd.DataFrame(metricas_prod)

# Top y Bottom 10 por RMSE
top10_rmse = df_metricas_prod.sort_values("RMSE").head(10)
bottom10_rmse = df_metricas_prod.sort_values("RMSE").tail(10)

print("\n===== 🔝 TOP 10 productos con mejor desempeño (RMSE) =====")
print(top10_rmse)

print("\n===== 🔻 BOTTOM 10 productos con peor desempeño (RMSE) =====")
print(bottom10_rmse)

print("\n==================================================================")

# Top y Bottom 10 por MAPE/sMAPE
top10_rmse = df_metricas_prod.sort_values("MAPE/sMAPE").head(10)
bottom10_rmse = df_metricas_prod.sort_values("MAPE/sMAPE").tail(10)

print("\n===== 🔝 TOP 10 productos con mejor desempeño (MAPE/sMAPE) =====")
print(top10_rmse)

print("\n===== 🔻 BOTTOM 10 productos con peor desempeño (MAPE/sMAPE) =====")
print(bottom10_rmse)

print("\n==================================================================")

# Top y Bottom 10 por R2
top10_rmse = df_metricas_prod.sort_values("R2").tail(10)
bottom10_rmse = df_metricas_prod.sort_values("R2").head(10)

print("\n===== 🔝 TOP 10 productos con mejor desempeño (R2) =====")
print(top10_rmse)

print("\n===== 🔻 BOTTOM 10 productos con peor desempeño (R2) =====")
print(bottom10_rmse)

# ===============================================================
# GRÁFICOS DE SERIES REALES VS PREDICHAS
# ===============================================================
productos = df_eval["Commodity"].unique()
for prod in productos:
    df_p = df_eval[df_eval["Commodity"] == prod]
    plt.figure(figsize=(10, 4))
    plt.plot(df_p["Date"], df_p["y_true_inv"], label="Real", lw=2)
    plt.plot(df_p["Date"], df_p["y_pred_inv"], label="Predicho", lw=2, linestyle="--")
    plt.title(f"{prod} - Predicción de precios (test)")
    plt.xlabel("Fecha")
    plt.ylabel("Precio")
    plt.legend()
    plt.tight_layout()
    plt.show()

"""##2.2 XGBoost por producto"""

# ==========================================
# 1. PARÁMETROS
# ==========================================
N_FOLDS = 7
UMBRAL_MIN = 1.0  # umbral para usar sMAPE
resultados = {}

# ==========================================
# 2. LOOP POR PRODUCTO
# ==========================================
for producto in sorted(X_train_valid["Commodity"].unique()):
    print(f"\n🔹 Entrenando modelo para: {producto}")

    # Filtrar datos del producto
    X_t_v = X_train_valid[X_train_valid["Commodity"] == producto].copy()
    y_t_v = y_train_valid[y_train_valid["Commodity"] == producto].copy()

    X_t_v = X_t_v.drop(columns=["Date", "Commodity"])
    y_t_v = y_t_v.drop(columns=["Commodity"])

    # Validación temporal tipo expanding window
    tscv = TimeSeriesSplit(n_splits=N_FOLDS)
    fold_metrics = []

    for fold, (train_idx, valid_idx) in enumerate(tscv.split(X_t_v)):
        X_train, X_valid = X_t_v.iloc[train_idx], X_t_v.iloc[valid_idx]
        y_train, y_valid = y_t_v.iloc[train_idx], y_t_v.iloc[valid_idx]

        # Modelo XGBoost compatible con Colab
        model = xgb.XGBRegressor(
            n_estimators=400,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="reg:squarederror",
            random_state=42,
            n_jobs=-1,
            #early_stopping_rounds=30, # removed early stopping rounds
            eval_metric="rmse"
        )

        # Entrenamiento
        model.fit(
            X_train, y_train,
            eval_set=[(X_valid, y_valid)],
            verbose=False
        )

        # Predicción
        y_pred = model.predict(X_valid)

        # Invertir transformaciones a precio real
        df_ref = train_valid[train_valid["Commodity"] == producto].iloc[valid_idx]
        y_true_inv = invertir_transformaciones(y_valid.values, df_ref)
        y_pred_inv = invertir_transformaciones(y_pred, df_ref)

        # Métricas por fold
        rmse = np.sqrt(mean_squared_error(y_true_inv, y_pred_inv))
        r2 = r2_score(y_true_inv, y_pred_inv)
        bias = f_mfe(y_true_inv, y_pred_inv)
        error_rel = mape_condicional(y_true_inv, y_pred_inv, UMBRAL_MIN)

        fold_metrics.append({
            "fold": fold,
            "RMSE": rmse,
            "R2": r2,
            "MFE": bias,
            "MAPE/sMAPE": error_rel
        })

        print(f"  Fold {fold}: RMSE={rmse:.2f}, MAPE/sMAPE={error_rel:.2f}, R2={r2:.2f}")

    # Entrenamiento final sobre todo train_valid
    model_final = xgb.XGBRegressor(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        random_state=42,
        n_jobs=-1,
        eval_metric="rmse"
    )
    model_final.fit(X_t_v, y_t_v)

    # Evaluación final sobre test
    X_test_prod = X_test[X_test["Commodity"] == producto].drop(columns=["Date", "Commodity"])
    y_test_prod = y_test[y_test["Commodity"] == producto]["resid_norm"]

    y_pred_test = model_final.predict(X_test_prod)
    df_test_ref = test[test["Commodity"] == producto]

    y_true_inv_test = invertir_transformaciones(y_test_prod.values, df_test_ref)
    y_pred_inv_test = invertir_transformaciones(y_pred_test, df_test_ref)

    rmse_test = np.sqrt(mean_squared_error(y_true_inv_test, y_pred_inv_test))
    r2_test = r2_score(y_true_inv_test, y_pred_inv_test)
    bias_test = f_mfe(y_true_inv_test, y_pred_inv_test)
    error_test = mape_condicional(y_true_inv_test, y_pred_inv_test, UMBRAL_MIN)

    # Guardar métricas y modelo
    df_folds = pd.DataFrame(fold_metrics)
    resultados[producto] = {
        "val_RMSE": df_folds["RMSE"].mean(),
        "val_MAPE/sMAPE": df_folds["MAPE/sMAPE"].mean(),
        "test_RMSE": rmse_test,
        "test_MAPE/sMAPE": error_test,
        "test_MFE": bias_test,
        "test_R2": r2_test,
        "modelo": model_final
    }

    # Guardar modelo individual
    joblib.dump(model_final, f"xgb_model_{producto.replace('/', '_')}.pkl")

# ==========================================
# Resumen final
# ==========================================

df_resultados = pd.DataFrame([
    {"Producto": k, **v} for k, v in resultados.items()
])

# Umbral de MAPE/sMAPE
UMBRAL_MAPE = 5.0

# Filtrar products that meet the condition
productos_filtrados = [p for p, r in resultados.items() if r["test_MAPE/sMAPE"] < UMBRAL_MAPE]

print(f"Productos con MAPE/sMAPE < {UMBRAL_MAPE}%: {productos_filtrados}")

# Loop para graficar
for producto_graf in productos_filtrados:
    df_test_ref = test[test["Commodity"] == producto_graf]
    X_test_prod = X_test[X_test["Commodity"] == producto_graf].drop(columns=["Date","Commodity"])

    y_true = invertir_transformaciones(
        y_test[y_test["Commodity"] == producto_graf]["resid_norm"].values,
        df_test_ref
    )
    y_pred = invertir_transformaciones(
        resultados[producto_graf]["modelo"].predict(X_test_prod),
        df_test_ref
    )

    residuos = y_true - y_pred


# Crear figura
plt.figure(figsize=(12,5))

# Precio real vs predicho
plt.subplot(1,2,1)
plt.plot(df_test_ref["Date"], y_true, label="Real", marker='o')
plt.plot(df_test_ref["Date"], y_pred, label="Predicho", marker='x')
plt.title(f"Precio real vs predicho - {producto_graf}")
plt.xlabel("Fecha")
plt.ylabel("Precio")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# Residuos
plt.subplot(1,2,2)
sns.histplot(residuos, bins=20, kde=True)
plt.title(f"Distribución de residuos - {producto_graf}")
plt.xlabel("Residuo (Real - Predicho)")
plt.ylabel("Frecuencia")
plt.grid(True)

plt.tight_layout()
plt.show()

#Graficos:import matplotlib.pyplot as plt
import seaborn as sns

# Producto que queremos graficar
producto_graf = "Apple(Jholey)"

# Extraer resultados del test
df_test_ref = test[test["Commodity"] == producto_graf]
y_true = invertir_transformaciones(y_test[y_test["Commodity"] == producto_graf]["resid_norm"].values, df_test_ref)
y_pred = invertir_transformaciones(resultados[producto_graf]["modelo"].predict(
    X_test[X_test["Commodity"] == producto_graf].drop(columns=["Date", "Commodity"])
), df_test_ref)

# Crear figura
plt.figure(figsize=(12,5))

# 1️⃣ Precio real vs predicho
plt.subplot(1,2,1)
plt.plot(df_test_ref["Date"], y_true, label="Real", marker='o')
plt.plot(df_test_ref["Date"], y_pred, label="Predicho", marker='x')
plt.title(f"Precio real vs predicho - {producto_graf}")
plt.xlabel("Fecha")
plt.ylabel("Precio")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)

# 2️⃣ Residuos
plt.subplot(1,2,2)
residuos = y_true - y_pred
sns.histplot(residuos, bins=20, kde=True)
plt.title(f"Distribución de residuos - {producto_graf}")
plt.xlabel("Residuo (Real - Predicho)")
plt.ylabel("Frecuencia")
plt.grid(True)

plt.tight_layout()
plt.show()

# Seleccionar los 10 productos con menor error en test
top_10 = df_resultados.sort_values("test_MAPE/sMAPE").head(10)["Producto"].tolist()

# Graficar cada uno
plt.figure(figsize=(6,3))
for producto in top_10:
    res = resultados[producto]
    modelo = res["modelo"]

    # Recuperar datos de test para ese producto
    data_p = df_feat[df_feat["Commodity"] == producto].copy().dropna()
    split_test = int(len(data_p) * 0.9)
    X_test = data_p.drop(columns=["Date", "Commodity", "Average", "Average_norm"]).iloc[split_test:]
    df_ref = data_p.iloc[split_test:]

    # Predicciones
    y_pred = modelo.predict(X_test)

    # Invertir transformaciones
    y_true = invertir_transformaciones(df_ref["Average_norm"], df_ref)
    y_pred_inv = invertir_transformaciones(y_pred, df_ref)

    # Gráfico
    plt.figure(figsize=(6,3))
    plt.plot(df_ref["Date"], y_true, label="Precio Real", color="black", linewidth=2)
    plt.plot(df_ref["Date"], y_pred_inv, label="Predicción XGBoost", color="tomato", linewidth=2, alpha=0.8)
    plt.title(f"Evolución del Precio - {producto}\n(MAPE/sMAPE: {res['test_MAPE/sMAPE']:.2f}%)", fontsize=8)
    plt.xlabel("Fecha", fontsize=6)
    plt.ylabel("Precio Promedio", fontsize=6)
    plt.legend(fontsize=6)
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.show()

"""##2.3 ARIMA

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --force-reinstall pmdarima

"""Please run the ARIMA code cell (`hxjqKxX56bPe`) again after the installation is complete. If the `ValueError` persists, you might need to restart the Colab runtime (`Runtime > Restart runtime`) and then re-run the cells from the beginning (skipping the initial library installations if they were successful)."""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pmdarima

"""After the installation is complete, please run the cell with the ARIMA code again."""

# ================================
# 1. Seleccionar los 80 productos con más datos
# ================================
conteo_productos = (
    df.groupby("Commodity")["Average"]
    .count()
    .reset_index(name="n_registros")
    .sort_values("n_registros", ascending=False)
)

top_80_productos = conteo_productos.head(80)["Commodity"].tolist()
df_top80 = df[df["Commodity"].isin(top_80_productos)].copy()
df_top80 = df_top80.sort_values(["Commodity", "Date"])

# ================================
# 2. Entrenar ARIMA por producto
# ================================
resultados_arima = {}

for producto in tqdm(top_80_productos):
    data_p = df_top80[df_top80["Commodity"] == producto].dropna()

    # Target
    y = data_p["Average"].values

    # Train-test split temporal (80/20)
    split = int(len(y) * 0.8)
    y_train, y_test = y[:split], y[split:]

    if len(y_train) < 50 or len(y_test) < 10:
        continue

    try:
        # AutoARIMA
        model = pm.auto_arima(
            y_train,
            seasonal=False,
            stepwise=True,
            suppress_warnings=True,
            error_action="ignore"
        )

        # Predicciones
        y_pred = model.predict(n_periods=len(y_test))

        # Métricas
        mape = mean_absolute_percentage_error(y_test, y_pred)
        rmse = math.sqrt(mean_squared_error(y_test, y_pred))

        resultados_arima[producto] = {
            "modelo": model,
            "mape": mape,
            "rmse": rmse,
            "y_test": y_test,
            "y_pred": y_pred
        }

    except Exception as e:
        print(f"Error con {producto}: {e}")
        continue

# ================================
# 3. Resumen de métricas
# ================================
df_arima_metrics = pd.DataFrame({
    "Producto": list(resultados_arima.keys()),
    "MAPE": [v["mape"] for v in resultados_arima.values()],
    "RMSE": [v["rmse"] for v in resultados_arima.values()]
})

print(df_arima_metrics.sort_values("MAPE"))
print("MAPE promedio total ARIMA:", df_arima_metrics["MAPE"].mean())
print("RMSE promedio total ARIMA:", df_arima_metrics["RMSE"].mean())

# Top 10 productos con menor MAPE
top10_arima = df_arima_metrics.sort_values("MAPE").head(10)["Producto"].tolist()

for producto in top10_arima:
    plt.figure(figsize=(12, 5))
    y_test = resultados_arima[producto]["y_test"]
    y_pred = resultados_arima[producto]["y_pred"]

    plt.plot(y_test, label="Valores reales", marker="o")
    plt.plot(y_pred, label="Predicción ARIMA", marker="x")
    plt.title(f"Predicción ARIMA - {producto}")
    plt.xlabel("Tiempo (test set)")
    plt.ylabel("Precio promedio")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Top 10 productos con menor MAPE
top10_arima = df_arima_metrics.sort_values("MAPE").head(10)["Producto"].tolist()

for producto in top10_arima:
    plt.figure(figsize=(12, 5))
    y_test = resultados_arima[producto]["y_test"]
    y_pred = resultados_arima[producto]["y_pred"]

    plt.plot(y_test, label="Valores reales", marker="o")
    plt.plot(y_pred, label="Predicción ARIMA", marker="x")
    plt.title(f"Predicción ARIMA - {producto}")
    plt.xlabel("Tiempo (test set)")
    plt.ylabel("Precio promedio")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

# Crear DataFrames comparativos
df_rf_metrics = pd.DataFrame({
    "Producto": list(resultados_rf.keys()),
    "MAPE_RF": [v["mape"] for v in resultados_rf.values()],
    "RMSE_RF": [v["rmse"] for v in resultados_rf.values()]
})

df_xgb_metrics = pd.DataFrame({
    "Producto": list(resultados_xgb.keys()),
    "MAPE_XGB": [v["mape"] for v in resultados_xgb.values()],
    "RMSE_XGB": [v["rmse"] for v in resultados_xgb.values()]
})

# Unir todo
df_comparacion = df_arima_metrics.merge(df_rf_metrics, on="Producto").merge(df_xgb_metrics, on="Producto")

print(df_comparacion.head())

# Gráfico comparativo MAPE promedio
mape_promedios = {
    "ARIMA": df_arima_metrics["MAPE"].mean(),
    "Random Forest": df_rf_metrics["MAPE_RF"].mean(),
    "XGBoost": df_xgb_metrics["MAPE_XGB"].mean()
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(mape_promedios.keys()), y=list(mape_promedios.values()), palette="viridis")
plt.title("Comparación de MAPE promedio entre modelos")
plt.ylabel("MAPE promedio")
plt.show()

# Gráfico comparativo RMSE promedio
rmse_promedios = {
    "ARIMA": df_arima_metrics["RMSE"].mean(),
    "Random Forest": df_rf_metrics["RMSE_RF"].mean(),
    "XGBoost": df_xgb_metrics["RMSE_XGB"].mean()
}

plt.figure(figsize=(8,5))
sns.barplot(x=list(rmse_promedios.keys()), y=list(rmse_promedios.values()), palette="magma")
plt.title("Comparación de RMSE promedio entre modelos")
plt.ylabel("RMSE promedio")
plt.show()

"""#3. PRODUCTO
1. Selección del modelo
2. Confección del producto
3. ¿Análisis de impacto esperado?
"""